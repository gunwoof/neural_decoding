"""
ConnecToMind2 - Model2 Implementation (BLIP-2 ITC+ITM Style)

BLIP-2 ê³µì‹ ë°©ì‹ ì™„ì „ ì ìš©:
    [ITC - Contrastive Learning]
    - FTCLoss: fMRI-Text(CLIP) Contrastive
    - Bidirectional: fMRIâ†’CLIP + CLIPâ†’fMRI
    - Max pooling over query tokens
    - Learnable temperature + label smoothing
    - In-batch negatives (ë‹¨ì¼ GPU ê¸°ì¤€)

    [ITM - Matching Classification]
    - FIMLoss: Binary classification (match/not match)
    - Classifier: nn.Linear(768, 2) - 2-class
    - Query ì‚¬ìš©: ëª¨ë“  100ê°œ query í† í° ì‚¬ìš©
    - Logit í‰ê· í™”: mean(dim=1) - ëª¨ë“  query í‰ê· 
    - Hard Negative Mining: FTC similarity ê¸°ë°˜ torch.multinomial sampling
    - 3B Triplet: pos+pos(matched), pos+neg_clip(unmatched), neg_fmri+pos(unmatched)

ì»¤ìŠ¤í…€ ìœ ì§€ (fMRI-Image Matching):
    - Attention Mask: Query+CLIP concat, use_mask=False (bidirectional)
    - Input: CLIP image embeddings [B, 257, 768] (BLIP-2ì˜ text ì—­í• )
    - Cross-attention: Query â† fMRI (BLIP-2ì˜ image ì—­í• )

Architecture (from diagram):
    fMRI [B, 100, (roi+padding)]
        -> (a) Region-level embedding -> [B, 100, 768]
        -> (b) Connectome-Q-former -> [B, 100, 768]
        -> Linear layer -> [B, 257, 768]
        -> L2 norm -> [B, 257, 768]
        -> Versatile Diffusion -> Reconstructed Image [B, 512, 512]

    Image [B, 224, 224]
        -> CLIP ViT-L/14 -> Last hidden [B, 257, 1024]
        -> Linear layer + L2 norm -> [B, 257, 768]

Loss = FIR Loss (fMRI embedding vs CLIP embedding, L1)
     + FTC Loss (BLIP-2 ITC style: bidirectional contrastive)
     + FIM Loss (BLIP-2 ITM style: 3B samples with hard negative mining)
     + Low-level Loss (L1 with target image)

Note: Low-level decoder outputs [B, 4, 32, 32] for 256x256 images.
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init

from transformers import CLIPVisionModel, CLIPVisionModelWithProjection
from diffusers import DiffusionPipeline
from diffusers.models.autoencoder_kl import Decoder


# ============================================================================
# CLIP Image Encoder (ê·¸ë¦¼ì˜ ì¢Œì¸¡ í•˜ë‹¨) - ViT-L/14
# ============================================================================

class CLIPImageEncoder(nn.Module):
    """
    CLIP ViT-L/14 ì´ë¯¸ì§€ ì¸ì½”ë”

    ê·¸ë¦¼ ì„¤ëª…:
        Image -> CLIP ViT-L/14 -> Last hidden [B, 257, 1280]
                               -> Linear layer + L2 norm -> [B, 257, 768]
                               -> CLS Token [B, 768] (for Cross Entropy Loss)
    """
    def __init__(self, pretrained_model="openai/clip-vit-large-patch14", freeze=True):
        super().__init__()
        # Use CLIPVisionModelWithProjection (includes pretrained visual_projection layer)
        self.clip_model = CLIPVisionModelWithProjection.from_pretrained(pretrained_model)

        if freeze:
            for param in self.clip_model.parameters():
                param.requires_grad = False

    def forward(self, images):
        """
        Input: images [B, 3, 224, 224]
        Output:
            hidden_state [B, 257, 768] - Pretrained projection + L2 norm
        """
        outputs = self.clip_model(images, output_hidden_states=True)
        last_hidden = outputs.last_hidden_state  # [B, 257, 1024]

        # Post layer norm
        last_hidden = self.clip_model.vision_model.post_layernorm(last_hidden)  # [B, 257, 1024]

        # Pretrained visual projection + L2 norm (MindEye1 style)
        hidden_state = self.clip_model.visual_projection(last_hidden)  # [B, 257, 768]
        hidden_state = F.normalize(hidden_state, dim=-1)  # L2 norm

        return hidden_state


# ============================================================================
# Region-level Embedding (ê·¸ë¦¼ì˜ (a))
# ============================================================================

class RegionLevelEmbedding(nn.Module):
    """
    (a) Region-level embedding

    ê·¸ë¦¼ ì„¤ëª…:
        task-fMRI [B, 100, (roi+padding)]
        -> Flatten -> Linear projection -> [B, 100, 768]
    """
    def __init__(self, seq_len=100, input_dim=3291, embed_dim=768):
        super().__init__()
        self.embed_dim = embed_dim
        self.seq_len = seq_len

        # Region-level embedding: ROIë³„ë¡œ ë‹¤ë¥¸ linear layer
        self.linear_weight = nn.Parameter(torch.empty(seq_len, input_dim, embed_dim))
        for t in range(seq_len):
            init.xavier_uniform_(self.linear_weight[t])

        self.layernorm = nn.LayerNorm(embed_dim)
        self.gelu = nn.GELU()
        self.dropout = nn.Dropout(0.1)

    def forward(self, x):
        """
        Input: x [B, 100, input_dim]
        Output: x [B, 100, 768]
        """
        # Ensure dtype compatibility with linear_weight
        x = x.to(dtype=self.linear_weight.dtype)

        # Region-level embedding (ê° ROIë³„ linear)
        x = torch.einsum("btd,tdh->bth", x, self.linear_weight)  # [B, 100, 768]
        x = self.layernorm(x)
        x = self.gelu(x)
        x = self.dropout(x)
        return x


# ============================================================================
# Connectome-Q-Former (ê·¸ë¦¼ì˜ (b))
# ============================================================================

class ConnectomeQFormerBlock(nn.Module):
    """
    Connectome-Q-Former ë¸”ë¡: Self-attention + Cross-attention (optional) + Feed forward

    ê·¸ë¦¼ ì„¤ëª…:
        Self-attention (ğŸ”¥ trainable)
        Cross-attention (ğŸ”¥ trainable) - 2 layerë§ˆë‹¤ í•œ ë²ˆ
        Feed forward (ğŸ”¥ trainable)
    """
    def __init__(self, hidden_size=768, num_heads=12, intermediate_size=3072,
                 dropout=0.1, layer_norm_eps=1e-6):
        super().__init__()

        # Self-Attention
        self.self_ln = nn.LayerNorm(hidden_size, eps=layer_norm_eps)
        self.self_attn = nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout, batch_first=True)
        self.self_drop = nn.Dropout(dropout)

        # Cross-Attention (query tokens attend to fMRI) - Queryë§Œ LayerNorm (BLIP-2 ë°©ì‹)
        self.cross_ln = nn.LayerNorm(hidden_size, eps=layer_norm_eps)
        self.cross_attn = nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout, batch_first=True)
        self.cross_drop = nn.Dropout(dropout)

        # Feed Forward
        self.ffn_ln = nn.LayerNorm(hidden_size, eps=layer_norm_eps)
        self.fc1 = nn.Linear(hidden_size, intermediate_size)
        self.fc2 = nn.Linear(intermediate_size, hidden_size)
        self.ffn_drop = nn.Dropout(dropout)
        self.activation = nn.GELU()

    def forward(self, x, fmri_feats, attn_mask=None, do_cross=True, n_q=None, cross_attn_mask=None):
        """
        Input:
            x [B, L, 768] - query tokens (+ optional CLIP hidden states)
                           L = n_q (inference) or n_q + n_t (training with mask)
            fmri_feats [B, 100, 768] - fMRI embeddings
            attn_mask: attention mask for Q-T separation
            do_cross: whether to do cross-attention in this layer
            n_q: number of query tokens (for extracting query part in cross-attention)
            cross_attn_mask: FC prior mask for cross-attention [B, n_q, seq_len]
        Output:
            x [B, L, 768]
        """
        # Self-Attention (with mask if provided)
        residual = x
        x_norm = self.self_ln(x)
        x_sa, _ = self.self_attn(x_norm, x_norm, x_norm,
                                  attn_mask=attn_mask, need_weights=False)
        x = residual + self.self_drop(x_sa)

        # Cross-Attention (query -> fMRI) - ì¡°ê±´ë¶€ ì‹¤í–‰, query ë¶€ë¶„ë§Œ
        if do_cross:
            q = x[:, :n_q, :]  #  Query ë¶€ë¶„ë§Œ cross-attention ì ìš© -> [B, n_q, 768]
            q_res = q
            q_norm = self.cross_ln(q)
            q_ca, _ = self.cross_attn(q_norm, fmri_feats, fmri_feats,
                                     attn_mask=cross_attn_mask,  # FC prior mask ì¶”ê°€
                                     need_weights=False)
            q = q_res + self.cross_drop(q_ca)
            x = torch.cat([q, x[:, n_q:, :]], dim=1)  # Query + ë‚˜ë¨¸ì§€ ë‹¤ì‹œ concat

        # Feed Forward
        residual = x
        x_norm = self.ffn_ln(x)
        x_ffn = self.fc2(self.ffn_drop(self.activation(self.fc1(x_norm))))
        x = residual + x_ffn  # dropoutì€ fc1â†’fc2 ì‚¬ì´ì—ë§Œ ì ìš© (BERT/GPT-2 ìŠ¤íƒ€ì¼)

        return x


class ConnectomeQFormer(nn.Module):
    """
    (b) Connectome-Q-Former (initialized from CLIP ViT-L/14)

    ê·¸ë¦¼ ì„¤ëª…:
        [B, 100, 768] (fMRI) + query tokens + CLIP hidden states
        -> Connectome-Q-Former blocks (with attention mask)
        -> [B, 100, 768]

    Query tokens: 100ê°œ (100 ROI)
    Weights initialized from openai/clip-vit-base-patch16
    Cross-attention: 2 layerë§ˆë‹¤ í•œ ë²ˆ (BLIP-2 ë°©ì‹)
    Layers: 12 (CLIP ViT-B/16 ê¸°ì¤€)

    ë§ˆìŠ¤í¬ ê¸°ë°˜ Self-attention (models.py ë°©ì‹):
        Query + CLIP hidden statesë¥¼ concatí•˜ì—¬ ì²˜ë¦¬
        ë§ˆìŠ¤í¬ë¡œ Q-T ìƒí˜¸ attend ì°¨ë‹¨ -> ë…ë¦½ì  ì²˜ë¦¬
    """
    def __init__(self, hidden_size=768, num_heads=12, num_layers=12,
                 num_query_tokens=100, dropout=0.1, cross_attention_freq=2,
                 clip_model_name="openai/clip-vit-base-patch16",
                 is_fc=False, subjects=None, fc_base_dir=None):
        super().__init__()

        self.hidden_size = hidden_size
        self.num_query_tokens = num_query_tokens
        self.num_layers = num_layers
        self.cross_attention_freq = cross_attention_freq
        self.is_fc = is_fc
        self.num_heads = num_heads

        # Learnable FC prior scaling parameter (ì´ˆê¸°ê°’ 1.0)
        self.fc_prior_scale = nn.Parameter(torch.ones(1))

        # Cross-attention ì ìš© ì—¬ë¶€ ë¯¸ë¦¬ ê³„ì‚° (0, 2, 4, ... ë²ˆì§¸ layer) - BLIP-2 ë°©ì‹
        self._do_cross_map = [(cross_attention_freq > 0) and (i % cross_attention_freq == 0)
                              for i in range(num_layers)]

        # Learnable query tokens [1, 100, 768] - position embedding ì—†ìŒ
        self.query_tokens = nn.Parameter(torch.randn(1, num_query_tokens, hidden_size))
        nn.init.normal_(self.query_tokens, std=0.02)

        # Connectome-Q-Former blocks (12 layers, CLIP ViT-B/16 ê¸°ì¤€)
        self.blocks = nn.ModuleList([
            ConnectomeQFormerBlock(hidden_size, num_heads, hidden_size * 4, dropout)
            for _ in range(num_layers)
        ])

        # Final LayerNorm
        self.final_ln = nn.LayerNorm(hidden_size)

        # FC prior ì´ˆê¸°í™” (subjectë³„ë¡œ ì €ì¥)
        self.fc_priors = {}  # Dict[subject_name, Tensor[100, 100]]
        if is_fc and subjects is not None and fc_base_dir is not None:
            self._load_fc_priors(subjects, fc_base_dir, num_query_tokens)

        # Initialize from CLIP weights
        self._init_from_clip(clip_model_name)

    def _init_from_clip(self, clip_model_name):
        """CLIP ViT-B/16ì—ì„œ weights ê°€ì ¸ì™€ì„œ ì´ˆê¸°í™” (Self-Attention + FFN)"""
        print(f"Initializing Q-Former from {clip_model_name}...")

        clip_model = CLIPVisionModel.from_pretrained(clip_model_name)
        clip_layers = clip_model.vision_model.encoder.layers

        # CLIP ViT-B/16: 12 layers, hidden_size=768 (ì°¨ì› ì¼ì¹˜!)
        for i, (block, clip_layer) in enumerate(zip(self.blocks, clip_layers)):
            # Self-Attention weights
            # CLIP: layer_norm1 -> self_attn
            block.self_ln.weight.data.copy_(clip_layer.layer_norm1.weight.data)
            block.self_ln.bias.data.copy_(clip_layer.layer_norm1.bias.data)

            # HuggingFace CLIP uses separate q_proj, k_proj, v_proj
            # PyTorch MultiheadAttention uses combined in_proj_weight [3*hidden, hidden]
            # Concatenate q, k, v weights into in_proj_weight
            q_weight = clip_layer.self_attn.q_proj.weight.data
            k_weight = clip_layer.self_attn.k_proj.weight.data
            v_weight = clip_layer.self_attn.v_proj.weight.data
            block.self_attn.in_proj_weight.data.copy_(torch.cat([q_weight, k_weight, v_weight], dim=0))

            q_bias = clip_layer.self_attn.q_proj.bias.data
            k_bias = clip_layer.self_attn.k_proj.bias.data
            v_bias = clip_layer.self_attn.v_proj.bias.data
            block.self_attn.in_proj_bias.data.copy_(torch.cat([q_bias, k_bias, v_bias], dim=0))

            block.self_attn.out_proj.weight.data.copy_(clip_layer.self_attn.out_proj.weight.data)
            block.self_attn.out_proj.bias.data.copy_(clip_layer.self_attn.out_proj.bias.data)

            # FFN weights
            # CLIP: layer_norm2 -> mlp (fc1 -> activation -> fc2)
            block.ffn_ln.weight.data.copy_(clip_layer.layer_norm2.weight.data)
            block.ffn_ln.bias.data.copy_(clip_layer.layer_norm2.bias.data)
            block.fc1.weight.data.copy_(clip_layer.mlp.fc1.weight.data)
            block.fc1.bias.data.copy_(clip_layer.mlp.fc1.bias.data)
            block.fc2.weight.data.copy_(clip_layer.mlp.fc2.weight.data)
            block.fc2.bias.data.copy_(clip_layer.mlp.fc2.bias.data)

            # Cross-Attentionì€ ëœë¤ ì´ˆê¸°í™” ìœ ì§€ (BLIP-2 ë°©ì‹)

        # Final LayerNorm from CLIP post_layernorm
        self.final_ln.weight.data.copy_(clip_model.vision_model.post_layernorm.weight.data)
        self.final_ln.bias.data.copy_(clip_model.vision_model.post_layernorm.bias.data)

        del clip_model
        print(f"âœ… Q-Former initialized from CLIP ViT-B/16 (12 layers, hidden_size=768)")

    def _load_fc_priors(self, subjects, fc_base_dir, num_query_tokens):
        """
        ëª¨ë“  subjectì˜ FC priorë¥¼ ë¡œë“œí•˜ê³  num_headsë§Œí¼ í™•ì¥í•˜ì—¬ ì €ì¥

        Args:
            subjects: list of subject names (e.g., ["sub-01", "sub-02", "sub-05", "sub-07"])
            fc_base_dir: base directory for FC prior files
            num_query_tokens: number of query tokens (100)

        FC prior shape: [100, 100] â†’ Cross-attention mask: [num_heads, 100, 100]
            - 100ê°œ ROI queriesì— FC prior ì§ì ‘ ì ìš©
            - num_heads ì°¨ì› ì¶”ê°€: ëª¨ë“  headì— ë™ì¼í•œ FC prior ì ìš©
        """
        import numpy as np

        seq_len = num_query_tokens  # 100 (CLS ì—†ìŒ)

        for sub in subjects:
            # FC prior íŒŒì¼ ê²½ë¡œ
            fc_path = f"{fc_base_dir}/{sub}/{sub}_FC_schaefer-100.npy"

            if not os.path.exists(fc_path):
                print(f"âš ï¸  FC prior not found: {fc_path}")
                continue

            # FC prior ë¡œë“œ: [100, 100]
            fc_prior = np.load(fc_path).astype(np.float32)

            assert fc_prior.shape[0] == fc_prior.shape[1] == seq_len, \
                f"FC prior shape mismatch for {sub}: {fc_prior.shape} vs {seq_len}"

            # Z-score ì •ê·œí™” (ì „ì²´): (fc_prior - mean) / std
            # ì „ì²´ 10,000ê°œ ê°’ì— ëŒ€í•´ í•˜ë‚˜ì˜ ë¶„í¬ë¡œ ì •ê·œí™”
            fc_mean = fc_prior.mean()  # scalar
            fc_std = fc_prior.std()    # scalar
            # stdê°€ 0ì´ ì•„ë‹ ë•Œë§Œ ì •ê·œí™”
            if fc_std > 1e-6:
                fc_prior = (fc_prior - fc_mean) / fc_std

            # Cross-attention mask ìƒì„±: [100, 100] - FC prior ì§ì ‘ ì‚¬ìš©
            mask = torch.from_numpy(fc_prior)  # [100, 100]

            # num_headsë§Œí¼ í™•ì¥: [num_heads, 100, 100]
            # ëª¨ë“  attention headì— ë™ì¼í•œ FC prior ì ìš©
            mask = mask.unsqueeze(0).repeat(self.num_heads, 1, 1)

            # ì €ì¥ (device ì´ë™ì€ forward ì‹œ ìˆ˜í–‰)
            self.fc_priors[sub] = mask

            print(f"âœ“ FC prior loaded for {sub}: shape={mask.shape}, range=[{fc_prior.min():.4f}, {fc_prior.max():.4f}]")

        print(f"âœ… Total {len(self.fc_priors)} FC priors loaded (pre-expanded for {self.num_heads} heads)")

    def build_mask(self, n_q, n_t, device):
        """
        Attention mask ìƒì„± (models.py ë°©ì‹)

             Q    T
        Q  [â–¡â–¡] [â– â– ]
        T  [â– â– ] [â–¡â–¡]

        â–¡: attend ê°€ëŠ¥ (False)
        â– : attend ë¶ˆê°€ (True)

        -> Që¼ë¦¬ë§Œ attend, Të¼ë¦¬ë§Œ attend (ìƒí˜¸ ì°¨ë‹¨)
        """
        L = n_q + n_t
        mask = torch.zeros(L, L, dtype=torch.bool, device=device)
        mask[:n_q, n_q:] = True  # Q -> T ì°¨ë‹¨
        mask[n_q:, :n_q] = True  # T -> Q ì°¨ë‹¨
        return mask

    def _build_fc_prior_mask(self, subject_names, device, dtype=None):
        """
        Batchì˜ ê° ìƒ˜í”Œì— ëŒ€í•´ í•´ë‹¹ subjectì˜ FC prior maskë¥¼ ìƒì„±

        Args:
            subject_names: list of subject names [B] (e.g., ["sub-01", "sub-02", ...])
            device: torch device
            dtype: torch dtype (for mixed precision compatibility)

        Returns:
            mask: [B*num_heads, 100, 100] - MultiheadAttention í˜•ì‹ì˜ FC prior mask (scaled)
        """
        masks = []
        for sub_name in subject_names:
            if sub_name in self.fc_priors:
                mask = self.fc_priors[sub_name].to(device=device, dtype=dtype if dtype else self.fc_priors[sub_name].dtype)
            else:
                # FC prior ì—†ìœ¼ë©´ zero mask (FC prior ì—†ì´ ë™ì‘)
                mask = torch.zeros(self.num_heads, self.num_query_tokens, self.num_query_tokens,
                                 dtype=dtype if dtype else torch.float32, device=device)
            masks.append(mask)

        # [B*num_heads, 100, 100] í˜•íƒœë¡œ concat
        fc_prior_mask = torch.cat(masks, dim=0)

        # Learnable scaling parameter ì ìš©
        fc_prior_mask = fc_prior_mask * self.fc_prior_scale

        return fc_prior_mask


    def forward(self, fmri_emb, clip_hidden=None, use_mask=True, subject_names=None):
        """
        Input:
            fmri_emb [B, 100, 768]
            clip_hidden [B, 257, 768] (optional) - CLIP hidden states for masked self-attention
            use_mask: True=Q-T ìƒí˜¸ ì°¨ë‹¨ (FIRìš©), False=ì „ë²”ìœ„ í—ˆìš© (FIMìš©)
            subject_names: list of subject names [B] (for FC prior)
        Output:
            query_output [B, 100, 768]
        """
        B = fmri_emb.size(0)
        device = fmri_emb.device

        # fMRI features (position embedding ì—†ìŒ)
        fmri_feats = fmri_emb  # [B, 100, 768]

        # Expand query tokens for batch (position embedding ì—†ìŒ)
        query = self.query_tokens.expand(B, -1, -1)  # [B, 100, 768]

        # Concat query + CLIP hidden states (if provided)
        if clip_hidden is not None:
            # [B, 100, 768] + [B, 257, 768] -> [B, 357, 768]
            x = torch.cat([query, clip_hidden], dim=1)
            n_q = self.num_query_tokens
            n_t = clip_hidden.size(1)
            # use_mask=False: ì „ë²”ìœ„ í—ˆìš© (FIMìš©) vs use_mask=True: Q-T ìƒí˜¸ ì°¨ë‹¨ (MSEìš©)
            attn_mask = self.build_mask(n_q, n_t, device) if use_mask else None
        else:
            x = query
            attn_mask = None

        # FC prior mask ìƒì„± (subjectë³„ë¡œ ë™ì  ìƒì„±)
        cross_attn_mask = None
        if self.is_fc and subject_names is not None:
            cross_attn_mask = self._build_fc_prior_mask(subject_names, device, dtype=fmri_feats.dtype)

        # Pass through Connectome-Q-Former blocks
        for i, block in enumerate(self.blocks):
            x = block(x, fmri_feats,
                     attn_mask=attn_mask,
                     do_cross=self._do_cross_map[i],
                     n_q=self.num_query_tokens,
                     cross_attn_mask=cross_attn_mask)

        # Extract query part only (if CLIP hidden was concatenated)
        if clip_hidden is not None:
            query_output = x[:, :self.num_query_tokens, :]  # [B, 100, 768]
        else:
            query_output = x

        # Final LayerNorm
        query_output = self.final_ln(query_output)  # [B, 100, 768]

        return query_output


# ============================================================================
# Low-Level Image Decoder
# ============================================================================

class LowLevelDecoder(nn.Module):
    """
    fMRI ì„ë² ë”©ì—ì„œ Low-level (blurry) ì´ë¯¸ì§€ ìƒì„± (MindEye2 ë°©ì‹)

    input:
        fmri_emb: [B, 100, 768]
    output:
        lowlevel_l1: [B, 4, 32, 32] - VAE latent space (256x256 ì´ë¯¸ì§€ ê¸°ì¤€)

    Shape íë¦„:
        [B, 100, 768] â†’ 76,800
            â†“ flatten
        [B, 76800] â†’ 76,800
            â†“ Linear (ì••ì¶•ë¥  18.75ë°°)
        [B, 4096] â†’ 64 Ã— 8 Ã— 8 = 4,096
            â†“ reshape
        [B, 64, 8, 8] â†’ 4,096
            â†“ UpBlock (8â†’16)
        [B, 64, 16, 16] â†’ 16,384
            â†“ UpBlock (16â†’32, ch: 64â†’4)
        [B, 4, 32, 32] â†’ 4,096  â† Versatile Diffusion latent (256x256)
    """
    def __init__(self, seq_len=100, embed_dim=768):
        super().__init__()

        self.flatten_dim = seq_len * embed_dim  # 100 * 768 = 76800

        # fmri_emb -> [B, 64, 8, 8] feature map (64*8*8 = 4096, ì••ì¶•ë¥  18.75ë°°)
        self.blin1 = nn.Linear(self.flatten_dim, 64 * 8 * 8, bias=True)
        self.bdropout = nn.Dropout(0.3)
        self.bnorm = nn.GroupNorm(1, 64)

        # [B, 64, 8, 8] -> [B, 4, 32, 32] (VAE latent space for 256x256, 2ê°œ UpBlockìœ¼ë¡œ 4x upsampling)
        # Note: Decoderì˜ ë§ˆì§€ë§‰ blockì€ upsample ì•ˆ í•¨, ê·¸ë˜ì„œ block_out_channelsëŠ” len(up_block_types)+1 í•„ìš”
        self.bupsampler = Decoder(
            in_channels=64,
            out_channels=4,
            up_block_types=["UpDecoderBlock2D", "UpDecoderBlock2D"],
            block_out_channels=[64, 64, 64],  # [32, 64, 128]ë³´ë‹¤ ë¹ ë¦„
            layers_per_block=1,
        )

    def forward(self, fmri_emb):
        """
        Input: fmri_emb [B, 100, 768]
        Output:
            lowlevel_l1: [B, 4, 32, 32] - VAE latent (for Versatile Diffusion 256x256)
        """
        B = fmri_emb.size(0)
        x = fmri_emb.view(B, -1)  # [B, 76800]

        # linear -> dropout -> reshape -> groupnorm
        lowlevel = self.blin1(x)  # [B, 4096]
        lowlevel = self.bdropout(lowlevel)
        lowlevel = lowlevel.reshape(B, 64, 8, 8).contiguous()  # [B, 64, 8, 8]
        lowlevel = self.bnorm(lowlevel)

        # VAE latent spaceë¡œ upsampling (256x256 ê¸°ì¤€)
        lowlevel_l1 = self.bupsampler(lowlevel)  # [B, 4, 32, 32]

        return lowlevel_l1


# ============================================================================
# Output Projection (Linear layer + L2 norm)
# ============================================================================

class OutputProjection(nn.Module):
    """
    Q-Former ì¶œë ¥ì„ CLIP spaceë¡œ projection (Transpose ë°©ì‹)

    ê·¸ë¦¼ ì„¤ëª…:
        Q-Former output [B, 100, 768]
        -> Transpose -> [B, 768, 100]
        -> Linear(100, 257) -> [B, 768, 257]
        -> Transpose -> [B, 257, 768]
        -> L2 norm -> [B, 257, 768]

    íŒŒë¼ë¯¸í„° ìˆ˜: 100 * 257 = 25,700 (Flatten ë°©ì‹ ëŒ€ë¹„ ~1,200,000ë°° ì ìŒ)
    """
    def __init__(self, input_tokens=100, output_tokens=257, hidden_size=768):
        super().__init__()
        self.input_tokens = input_tokens
        self.output_tokens = output_tokens
        self.hidden_size = hidden_size

        # [B, 768, 100] -> [B, 768, 257] (ê° dimensionë³„ ë…ë¦½ projection)
        self.proj = nn.Linear(input_tokens, output_tokens)

    def forward(self, x):
        """
        Input: x [B, 100, 768]
        Output: x [B, 257, 768] (L2 normalized)
        """
        x = x.transpose(1, 2)  # [B, 768, 100]
        x = self.proj(x)       # [B, 768, 257]
        x = x.transpose(1, 2)  # [B, 257, 768]
        x = F.normalize(x, dim=-1)  # L2 norm
        return x


# ============================================================================
# Loss Functions
# ============================================================================

class FIRLoss(nn.Module):
    """
    FIR Loss (fMRI-Image Reconstruction): fMRI embedding vs CLIP embedding

    ê·¸ë¦¼ ì„¤ëª…:
        Linear layer [B, 257, 768] <------ FIR Loss ------> Linear layer [B, 257, 768]
        (from Q-Former)                                     (from CLIP)

    L1 Loss ì‚¬ìš©: MSEë³´ë‹¤ gradientê°€ ì•ˆì •ì  (í° ì˜¤ì°¨ì—ì„œë„ gradientê°€ ì¼ì •)
    """
    def __init__(self):
        super().__init__()
        self.l1 = nn.L1Loss()

    def forward(self, fmri_emb, clip_emb):
        """
        Input:
            fmri_emb [B, 257, 768] - Q-Former output (L2 normalized)
            clip_emb [B, 257, 768] - CLIP output (L2 normalized)
        Output: scalar loss
        """
        return self.l1(fmri_emb, clip_emb)


class FTCLoss(nn.Module):
    """
    FTC Loss (fMRI-Text(CLIP) Contrastive): BLIP-2 ITC ë°©ì‹

    BLIP-2 ITC êµ¬í˜„:
        1. fMRI features (all tokens) vs CLIP features (CLS token) similarity ê³„ì‚°
        2. Max pooling over fMRI tokens (32ê°œ query ì¤‘ ìµœëŒ€ê°’ ì„ íƒ)
        3. Bidirectional contrastive loss (fMRIâ†’CLIP + CLIPâ†’fMRI)
        4. In-batch negatives (ë‹¨ì¼ GPU ê¸°ì¤€)
        5. Temperature scaling (learnable parameter)
        6. Label smoothing (0.1)

    Shapes:
        fmri_proj: [B, 257, 768] - fMRI embeddings (all tokens)
        clip_proj: [B, 257, 768] - CLIP embeddings (use CLS token [B, 768])
        sim_fmri2clip: [B, B] - fMRIâ†’CLIP similarity (after max pooling)
        sim_clip2fmri: [B, B] - CLIPâ†’fMRI similarity (after max pooling)
    """
    def __init__(self, temperature=0.07, label_smoothing=0.1):
        super().__init__()
        # Learnable temperature parameter (BLIP-2 style)
        self.temp = nn.Parameter(torch.ones([]) * temperature)
        self.label_smoothing = label_smoothing

    def forward(self, fmri_proj, clip_proj):
        """
        Input:
            fmri_proj [B, 257, 768] - fMRI embeddings (L2 normalized)
            clip_proj [B, 257, 768] - CLIP embeddings (L2 normalized)
        Output:
            loss_ftc: scalar
            sim_fmri2clip: [B, B] - for hard negative mining in FIM
            sim_clip2fmri: [B, B] - for hard negative mining in FIM
        """
        B = fmri_proj.size(0)
        device = fmri_proj.device

        # Normalize features
        fmri_norm = F.normalize(fmri_proj, dim=-1)  # [B, 257, 768]
        clip_cls_norm = F.normalize(clip_proj[:, 0, :], dim=-1)  # [B, 768] - CLS only

        # Compute similarity: fMRI (all tokens) vs CLIP (CLS)
        # einsum: 'bid,cd->bic' where b=fmri_batch, i=fmri_tokens, d=dim, c=clip_batch
        sim_q2t = torch.einsum('bid,cd->bic', fmri_norm, clip_cls_norm)
        # [B, 257, B] - (i-th fmri's each token) vs (all clip CLS)

        # Max over fmri tokens: [B, 257, B] -> [B, B]
        sim_fmri2clip, _ = sim_q2t.max(dim=1)  # [B, B]

        # Symmetric: CLIP (CLS) vs fMRI (all tokens)
        # einsum: 'bd,cid->bci' where b=clip_batch, d=dim, c=fmri_batch, i=fmri_tokens
        sim_t2q = torch.einsum('bd,cid->bci', clip_cls_norm, fmri_norm)
        # [B, B, 257] - (i-th clip CLS) vs (all fmri's each token)

        # Max over fmri tokens: [B, B, 257] -> [B, B]
        sim_clip2fmri, _ = sim_t2q.max(dim=-1)  # [B, B]

        # Temperature scaling (learnable)
        sim_fmri2clip = sim_fmri2clip / self.temp
        sim_clip2fmri = sim_clip2fmri / self.temp

        # Targets: diagonal (positive pairs)
        targets = torch.arange(B, device=device, dtype=torch.long)

        # Bidirectional contrastive loss (BLIP-2 ITC style)
        loss_ftc = (
            F.cross_entropy(sim_fmri2clip, targets, label_smoothing=self.label_smoothing)
            + F.cross_entropy(sim_clip2fmri, targets, label_smoothing=self.label_smoothing)
        ) / 2

        return loss_ftc, sim_fmri2clip, sim_clip2fmri


class FIMLoss(nn.Module):
    """
    FIM Loss (fMRI-Image Matching): BLIP-2 ITM ë°©ì‹ with Hard Negative Mining

    BLIP-2 ITM êµ¬í˜„:
        1. ITC similarityë¥¼ ì‚¬ìš©í•˜ì—¬ hard negative sampling
        2. 3B triplet êµ¬ì„±: (pos+pos, pos+neg_clip, neg_fmri+pos)
        3. Q-Former forward with 3B samples (use_mask=False)
        4. Binary classification (match=1, not match=0)
        5. All query tokens ì‚¬ìš© + mean pooling

    Hard Negative Mining:
        - sim_fmri2clip, sim_clip2fmriì—ì„œ diagonal ì œì™¸
        - Softmaxë¡œ í™•ë¥  ë¶„í¬ ë³€í™˜
        - torch.multinomialë¡œ hard negative sampling

    Shapes:
        fmri_emb: [B, 100, 768] - Region-level embeddings
        clip_proj: [B, 257, 768] - CLIP embeddings
        sim_fmri2clip: [B, B] - fMRIâ†’CLIP similarity (from FTC)
        sim_clip2fmri: [B, B] - CLIPâ†’fMRI similarity (from FTC)
        Output: scalar loss
    """
    def __init__(self, qformer, fim_classifier):
        super().__init__()
        self.qformer = qformer
        self.fim_classifier = fim_classifier

    def forward(self, fmri_emb, clip_proj, sim_fmri2clip, sim_clip2fmri, subject_names=None):
        """
        Input:
            fmri_emb [B, 100, 768] - Region-level embeddings
            clip_proj [B, 257, 768] - CLIP embeddings
            sim_fmri2clip [B, B] - fMRIâ†’CLIP similarity (from FTC)
            sim_clip2fmri [B, B] - CLIPâ†’fMRI similarity (from FTC)
            subject_names: list of subject names [B] (for FC prior)
        Output:
            loss_fim: scalar
        """
        B = fmri_emb.size(0)
        device = fmri_emb.device

        # Step 1: Hard negative mining (BLIP-2 style: within torch.no_grad)
        with torch.no_grad():
            # Mask diagonal (positive pairs)
            sim_fmri2clip_neg = sim_fmri2clip.clone()
            sim_clip2fmri_neg = sim_clip2fmri.clone()
            sim_fmri2clip_neg.fill_diagonal_(-10000)
            sim_clip2fmri_neg.fill_diagonal_(-10000)

            # Softmax to get sampling weights
            weights_fmri2clip = F.softmax(sim_fmri2clip_neg, dim=1)  # [B, B]
            weights_clip2fmri = F.softmax(sim_clip2fmri_neg, dim=1)  # [B, B]

        # Step 2: Sample hard negatives with torch.multinomial
        # Hard negative clip for each fmri
        clip_neg_indices = []
        for b in range(B):
            neg_idx = torch.multinomial(weights_fmri2clip[b], 1).item()
            clip_neg_indices.append(neg_idx)
        clip_neg_indices = torch.tensor(clip_neg_indices, device=device, dtype=torch.long)
        clip_proj_neg = clip_proj[clip_neg_indices]  # [B, 257, 768]

        # Hard negative fmri for each clip
        fmri_neg_indices = []
        for b in range(B):
            neg_idx = torch.multinomial(weights_clip2fmri[b], 1).item()
            fmri_neg_indices.append(neg_idx)
        fmri_neg_indices = torch.tensor(fmri_neg_indices, device=device, dtype=torch.long)
        fmri_emb_neg = fmri_emb[fmri_neg_indices]  # [B, 100, 768]

        # Step 4: Construct 3B samples (BLIP-2 triplet style)
        # 1) fmri(pos) + clip(pos)  â†’ matched (label=1)
        # 2) fmri(pos) + clip(neg)  â†’ unmatched (label=0)
        # 3) fmri(neg) + clip(pos)  â†’ unmatched (label=0)
        fmri_emb_3b = torch.cat([
            fmri_emb,      # [B, 100, 768] - positive
            fmri_emb,      # [B, 100, 768] - positive (for negative clip)
            fmri_emb_neg,  # [B, 100, 768] - negative fmri
        ], dim=0)  # [3B, 100, 768]

        clip_proj_3b = torch.cat([
            clip_proj,      # [B, 257, 768] - positive
            clip_proj_neg,  # [B, 257, 768] - negative clip
            clip_proj,      # [B, 257, 768] - positive (for negative fmri)
        ], dim=0)  # [3B, 257, 768]

        fim_labels = torch.cat([
            torch.ones(B, device=device),   # [B] - matched
            torch.zeros(2 * B, device=device),  # [2B] - unmatched
        ], dim=0)  # [3B]

        # Subject names 3ë°°ë¡œ ë³µì œ
        if subject_names is not None:
            subject_names_neg = [subject_names[i] for i in fmri_neg_indices.cpu().tolist()]
            subject_names_3b = subject_names + subject_names + subject_names_neg
        else:
            subject_names_3b = None

        # Step 5: Q-Former forward with 3B samples (use_mask=False)
        qformer_out_fim = self.qformer(
            fmri_emb_3b, clip_proj_3b, use_mask=False, subject_names=subject_names_3b
        )  # [3B, 100, 768]

        # Step 6: Binary classification (BLIP-2 ITM style)
        vl_embeddings = qformer_out_fim  # [3B, 100, 768] - ëª¨ë“  query ì‚¬ìš©
        vl_output = self.fim_classifier(vl_embeddings)  # [3B, 100, 2] - ê° queryì—ì„œ 2-class
        fim_logits = vl_output.mean(dim=1)  # [3B, 2] - ëª¨ë“  query í‰ê· í™”

        # Cross entropy loss (2-class)
        loss_fim = F.cross_entropy(fim_logits, fim_labels.long())

        return loss_fim


# ============================================================================
# Complete Model (BLIP-2 ITM Style)
# ============================================================================

class ConnecToMind2(nn.Module):
    """
    ConnecToMind2 - Model2 (BLIP-2 ITC+ITM Style)

    BLIP-2 ë°©ì‹ ì™„ì „ ì ìš©:
        [FTCLoss - Contrastive Learning (ITC ê¸°ë°˜)]
        - Bidirectional contrastive loss (fMRIâ†’CLIP + CLIPâ†’fMRI)
        - Max pooling over query tokens
        - Learnable temperature + label smoothing
        - In-batch negatives (ë‹¨ì¼ GPU ê¸°ì¤€)

        [FIMLoss - Matching Classification (ITM ê¸°ë°˜)]
        - Classifier: nn.Linear(768, 2) - 2-class
        - Query ì‚¬ìš©: ëª¨ë“  100ê°œ query í† í° ì‚¬ìš©
        - Logit í‰ê· í™”: mean(dim=1) - ëª¨ë“  query í‰ê· 
        - Hard Negative Mining: FTC similarity ê¸°ë°˜ sampling
        - 3B Triplet: pos+pos(1), pos+neg_clip(0), neg_fmri+pos(0)

    Architecture:
        fMRI [B, 100, input_dim]
            -> (a) Region-level embedding -> [B, 100, 768]
            -> (b) Connectome-Q-Former -> [B, 100, 768]
            -> Linear layer -> [B, 257, 768]
            -> L2 norm -> [B, 257, 768]

        Image [B, 3, 224, 224]
            -> CLIP ViT-L/14 -> [B, 257, 1024]
            -> Linear layer + L2 norm -> [B, 257, 768]

    Loss = FIR Loss (fMRI embedding vs CLIP embedding, L1)
         + FTC Loss (BLIP-2 ITC style: bidirectional contrastive)
         + FIM Loss (BLIP-2 ITM style: 3B samples with hard negatives)
         + Low-level Loss (VAE L1)

    Output (training):
        fmri_proj: [B, 257, 768] - Q-Former output (Linear + L2 norm)
        clip_proj: [B, 257, 768] - CLIP output (Linear + L2 norm)
        lowlevel_l1: [B, 4, 32, 32] - VAE latent (for Versatile Diffusion 512x512)
        loss_fir: FIR loss
        loss_ftc: FTC loss (ITC-based contrastive)
        loss_fim: FIM loss (3B samples, hard negative mining)

    Versatile Diffusion inputs:
        - image_embeds: fmri_proj [B, 257, 768]
        - image: VAE decode(lowlevel_l1) for image condition (256x256)
    """
    def __init__(self, seq_len=100, input_dim=3291, embed_dim=768,
                 num_qformer_layers=12, num_query_tokens=100,
                 is_fc=False, subjects=None, fc_base_dir=None):
        super().__init__()

        self.seq_len = seq_len
        self.embed_dim = embed_dim
        self.num_query_tokens = num_query_tokens

        # 1. CLIP Image Encoder (ViT-L/14, frozen)
        self.clip_encoder = CLIPImageEncoder(freeze=True)

        # 2. Region-level Embedding
        self.region_embedding = RegionLevelEmbedding(
            seq_len=seq_len,
            input_dim=input_dim,
            embed_dim=embed_dim
        )

        # 3. Connectome-Q-Former (initialized from CLIP, with FC prior)
        self.connectome_qformer = ConnectomeQFormer(
            hidden_size=embed_dim,
            num_heads=12,
            num_layers=num_qformer_layers,
            num_query_tokens=num_query_tokens,
            dropout=0.1,
            is_fc=is_fc,
            subjects=subjects,
            fc_base_dir=fc_base_dir
        )

        # 4. Output Projection: [B, 100, 768] -> [B, 257, 768]
        self.output_proj = OutputProjection(
            input_tokens=num_query_tokens,
            output_tokens=257,
            hidden_size=embed_dim
        )

        # 5. Low-Level Decoder
        self.low_level_decoder = LowLevelDecoder(seq_len=seq_len, embed_dim=embed_dim)

        # 6. FIM classifier (BLIP-2 ITM Style): ëª¨ë“  query [768] -> 2-class logit [2]
        self.fim_classifier = nn.Linear(embed_dim, 2)

        # 7. Loss functions
        self.fir_loss_fn = FIRLoss()
        self.ftc_loss_fn = FTCLoss(temperature=0.07, label_smoothing=0.1)
        self.fim_loss_fn = FIMLoss(qformer=self.connectome_qformer, fim_classifier=self.fim_classifier)

    def forward(self, fmri, images, device, subject_names=None):
        """
        Training forward (Q-Former 2ë²ˆ í˜¸ì¶œ: FIRìš© ë§ˆìŠ¤í¬ O, FIMìš© ë§ˆìŠ¤í¬ X)

        FIR: Q-T ìƒí˜¸ ì°¨ë‹¨ ë§ˆìŠ¤í¬ ì‚¬ìš© (Queryê°€ CLIPì„ ì§ì ‘ ë³´ë©´ cheating)
        FTC: ITC ê¸°ë°˜ contrastive learning (fMRI-CLIP alignment)
        FIM: ë§ˆìŠ¤í¬ ì—†ìŒ (Queryê°€ CLIPì„ ë³´ê³  matching íŒë‹¨)
             Hard Negative Mining + 3B samples (BLIP-2 ITM ê³µì‹ ë°©ì‹)

        Loss Classes:
            - FIRLoss: L1 loss (fmri_proj vs clip_proj)
            - FTCLoss: Bidirectional contrastive loss (ITC ë°©ì‹)
            - FIMLoss: Binary classification with hard negative mining (ITM ë°©ì‹)

        Input:
            fmri [B, 100, input_dim]
            images [B, 3, 224, 224]
            device: torch device
            subject_names: list of subject names [B] (for FC prior)

        Output: dict
            fmri_proj: [B, 257, 768] - for FIR loss
            clip_proj: [B, 257, 768] - for FIR loss
            lowlevel_l1: [B, 4, 32, 32] - for Versatile Diffusion 512x512
            loss_fir: scalar
            loss_ftc: scalar (ITC-based contrastive)
            loss_fim: scalar (BLIP-2 ITM style with 3B samples)
        """
        B = fmri.size(0)

        # === Image path (CLIP) ===
        clip_proj = self.clip_encoder(images)  # [B, 257, 768]

        # === fMRI path ===
        # (a) Region-level embedding
        fmri_emb = self.region_embedding(fmri)  # [B, 100, 768]

        # === FIR Branch (ë§ˆìŠ¤í¬ O: Q-T ìƒí˜¸ ì°¨ë‹¨) ===
        qformer_out_fir = self.connectome_qformer(fmri_emb, clip_proj, use_mask=True, subject_names=subject_names)  # [B, 100, 768]
        fmri_proj = self.output_proj(qformer_out_fir)  # [B, 257, 768]

        # === Compute Losses ===

        # 1. FIR Loss (L1)
        loss_fir = self.fir_loss_fn(fmri_proj, clip_proj)

        # 2. FTC Loss (ITC-based Contrastive) - returns similarity matrices for hard negative mining
        loss_ftc, sim_fmri2clip, sim_clip2fmri = self.ftc_loss_fn(fmri_proj, clip_proj)

        # 3. FIM Loss (ITM with Hard Negative Mining)
        loss_fim = self.fim_loss_fn(fmri_emb, clip_proj, sim_fmri2clip, sim_clip2fmri, subject_names=subject_names)

        # 4. Low-level decoder (ì›ë³¸ fmri_embë§Œ ì‚¬ìš©)
        lowlevel_l1 = self.low_level_decoder(fmri_emb)  # [B, 4, 64, 64]

        return {
            "fmri_proj": fmri_proj,          # [B, 257, 768]
            "clip_proj": clip_proj,          # [B, 257, 768]
            "lowlevel_l1": lowlevel_l1,      # [B, 4, 64, 64]
            "loss_fir": loss_fir,
            "loss_ftc": loss_ftc,
            "loss_fim": loss_fim,
        }

    def inference(self, fmri, subject_names=None):
        """
        Inference (ì´ë¯¸ì§€ ì—†ì´)

        Input:
            fmri [B, 100, input_dim]
            subject_names: list of subject names [B] (for FC prior)
        Output:
            fmri_proj: [B, 257, 768] - Versatile Diffusionì˜ image_embedsë¡œ ì‚¬ìš©
            lowlevel_l1: [B, 4, 32, 32] - Versatile Diffusion latent (512x512)
        """
        # (a) Region-level embedding
        fmri_emb = self.region_embedding(fmri)  # [B, 100, 768]

        # (b) Connectome-Q-Former
        qformer_out = self.connectome_qformer(fmri_emb, subject_names=subject_names)  # [B, 100, 768]

        # Linear layer + L2 norm
        fmri_proj = self.output_proj(qformer_out)  # [B, 257, 768]

        # Low-level decoder
        lowlevel_l1 = self.low_level_decoder(fmri_emb)  # [B, 4, 64, 64]

        return {
            "fmri_proj": fmri_proj,      # [B, 257, 768]
            "lowlevel_l1": lowlevel_l1,  # [B, 4, 64, 64]
        }


# ============================================================================
# Model Factory
# ============================================================================

def get_model(args):
    """
    ëª¨ë¸ ìƒì„±

    args í•„ìš” ì†ì„±:
        - seq_len, input_dim, embed_dim, num_qformer_layers, num_query_tokens
        - cache_dir: pretrained model cache ê²½ë¡œ

    returns:
        connectomind2: ë©”ì¸ ëª¨ë¸
        versatile_diffusion: Versatile Diffusion pipeline
        vae: VAE encoder (for L1 loss)
        l1: L1 loss function
    """
    # ìºì‹œ ê²½ë¡œ (ë¡œì»¬ ìš°ì„ , ì—†ìœ¼ë©´ ì˜¨ë¼ì¸ì—ì„œ ë‹¤ìš´ë¡œë“œ)
    cache_dir = args.cache_dir

    # FC prior base directory (is_fcê°€ Trueì¸ ê²½ìš°ì—ë§Œ ì‚¬ìš©)
    fc_base_dir = None
    if args.is_fc:
        fc_base_dir = f"{args.root_dir}/{args.fmri_dir}/{args.fmri_detail_dir}"

    # ë©”ì¸ ëª¨ë¸
    connectomind2 = ConnecToMind2(
        seq_len=args.seq_len,
        input_dim=args.input_dim,
        embed_dim=args.embed_dim,
        num_qformer_layers=args.num_qformer_layers,
        num_query_tokens=args.num_query_tokens,
        is_fc=args.is_fc,
        subjects=args.subjects,
        fc_base_dir=fc_base_dir,
    )

    # High-level reconstruction ìš©ë„ -> Versatile Diffusion pipeline
    # ë¡œì»¬ì— ìˆìœ¼ë©´ ë¡œì»¬ì—ì„œ, ì—†ìœ¼ë©´ ì˜¨ë¼ì¸ì—ì„œ ë‹¤ìš´ë¡œë“œ í›„ cache_dirì— ì €ì¥
    print("Loading Versatile Diffusion pipeline...")
    try:
        versatile_diffusion = DiffusionPipeline.from_pretrained(
            "shi-labs/versatile-diffusion",
            torch_dtype=torch.float16,
            cache_dir=cache_dir,
            local_files_only=True  # ë¡œì»¬ ìš°ì„  ì‹œë„
        )
        print("âœ… Versatile Diffusion loaded (from local cache)")
    except Exception:
        print("  ë¡œì»¬ ìºì‹œ ì—†ìŒ, ì˜¨ë¼ì¸ì—ì„œ ë‹¤ìš´ë¡œë“œ...")
        try:
            versatile_diffusion = DiffusionPipeline.from_pretrained(
                "shi-labs/versatile-diffusion",
                torch_dtype=torch.float16,
                cache_dir=cache_dir
            )
            print("âœ… Versatile Diffusion downloaded and cached")
        except Exception as e:
            print(f"[!] Versatile Diffusion ë¡œë”© ì‹¤íŒ¨: {e}")
            versatile_diffusion = None

    # Low-level reconstruction ìš©ë„ -> VAE (for L1 loss - low-level)
    print("Loading VAE...")
    try:
        sd_pipe = DiffusionPipeline.from_pretrained(
            "lambdalabs/sd-image-variations-diffusers",
            cache_dir=cache_dir,
            local_files_only=True  # ë¡œì»¬ ìš°ì„  ì‹œë„
        )
        print("âœ… VAE loaded (from local cache)")
    except Exception:
        print("  ë¡œì»¬ ìºì‹œ ì—†ìŒ, ì˜¨ë¼ì¸ì—ì„œ ë‹¤ìš´ë¡œë“œ...")
        sd_pipe = DiffusionPipeline.from_pretrained(
            "lambdalabs/sd-image-variations-diffusers",
            cache_dir=cache_dir
        )
        print("âœ… VAE downloaded and cached")
    vae = sd_pipe.vae
    vae.eval().requires_grad_(False)

    # L1 loss
    l1 = nn.L1Loss()

    return {
        "connectomind2": connectomind2,
        "versatile_diffusion": versatile_diffusion,
        "vae": vae,
        "l1": l1,
    }
