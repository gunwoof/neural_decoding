"""
ConnecToMind2 - Model2 Implementation (Based on New Architecture Diagram)

Architecture (from diagram):
    fMRI [B, 100, (roi+padding)]
        -> (a) Region-level embedding -> [B, 100, 768]
        -> (b) Connectome-Q-former -> [B, 101, 768]
        -> Linear layer -> [B, 257, 768]
        -> L2 norm -> [B, 257, 768]
        -> Versatile Diffusion -> Reconstructed Image [B, 512, 512]

    Image [B, 224, 224]
        -> CLIP ViT-L/14 -> Last hidden [B, 257, 1024]
        -> Linear layer + L2 norm -> [B, 257, 768]

Loss = FIR Loss (fMRI embedding vs CLIP embedding, MSE)
     + Cross Entropy Loss (CLS token)
     + Low-level Loss (L1 with target image)
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init

from transformers import CLIPVisionModel
from diffusers import DiffusionPipeline
from diffusers.models.autoencoder_kl import Decoder


# ============================================================================
# CLIP Image Encoder (ê·¸ë¦¼ì˜ ì¢Œì¸¡ í•˜ë‹¨) - ViT-L/14
# ============================================================================

class CLIPImageEncoder(nn.Module):
    """
    CLIP ViT-L/14 ì´ë¯¸ì§€ ì¸ì½”ë”

    ê·¸ë¦¼ ì„¤ëª…:
        Image -> CLIP ViT-L/14 -> Last hidden [B, 257, 1280]
                               -> Linear layer + L2 norm -> [B, 257, 768]
                               -> CLS Token [B, 768] (for Cross Entropy Loss)
    """
    def __init__(self, pretrained_model="openai/clip-vit-large-patch14", freeze=True):
        super().__init__()
        self.clip_model = CLIPVisionModel.from_pretrained(pretrained_model)

        if freeze:
            for param in self.clip_model.parameters():
                param.requires_grad = False

        # CLIP ViT-L/14 hidden size: 1024
        self.clip_hidden_size = 1024
        self.output_dim = 768

        # Linear layer: [B, 257, 1024] -> [B, 257, 768]
        self.proj = nn.Linear(self.clip_hidden_size, self.output_dim)

    def forward(self, images):
        """
        Input: images [B, 3, 224, 224]
        Output:
            hidden_state [B, 257, 768] - Linear + L2 norm (for MSE Loss)
            cls_token [B, 768] - CLS token (for Cross Entropy Loss)
        """
        outputs = self.clip_model(images, output_hidden_states=True)
        last_hidden = outputs.last_hidden_state  # [B, 257, 1024]

        # Linear layer + L2 norm
        hidden_state = self.proj(last_hidden)  # [B, 257, 768]
        hidden_state = F.normalize(hidden_state, dim=-1)  # L2 norm

        return hidden_state


# ============================================================================
# Region-level Embedding (ê·¸ë¦¼ì˜ (a))
# ============================================================================

class RegionLevelEmbedding(nn.Module):
    """
    (a) Region-level embedding

    ê·¸ë¦¼ ì„¤ëª…:
        task-fMRI [B, 100, (roi+padding)]
        -> Flatten -> Linear projection -> [B, 100, 768]
    """
    def __init__(self, seq_len=100, input_dim=3291, embed_dim=768):
        super().__init__()
        self.embed_dim = embed_dim
        self.seq_len = seq_len

        # Region-level embedding: ROIë³„ë¡œ ë‹¤ë¥¸ linear layer
        self.linear_weight = nn.Parameter(torch.empty(seq_len, input_dim, embed_dim))
        for t in range(seq_len):
            init.xavier_uniform_(self.linear_weight[t])

        self.layernorm = nn.LayerNorm(embed_dim)
        self.gelu = nn.GELU()
        self.dropout = nn.Dropout(0.1)

    def forward(self, x):
        """
        Input: x [B, 100, input_dim]
        Output: x [B, 100, 768]
        """
        # Ensure dtype compatibility with linear_weight
        x = x.to(dtype=self.linear_weight.dtype)

        # Region-level embedding (ê° ROIë³„ linear)
        x = torch.einsum("btd,tdh->bth", x, self.linear_weight)  # [B, 100, 768]
        x = self.layernorm(x)
        x = self.gelu(x)
        x = self.dropout(x)
        return x


# ============================================================================
# Connectome-Q-Former (ê·¸ë¦¼ì˜ (b))
# ============================================================================

class ConnectomeQFormerBlock(nn.Module):
    """
    Connectome-Q-Former ë¸”ë¡: Self-attention + Cross-attention (optional) + Feed forward

    ê·¸ë¦¼ ì„¤ëª…:
        Self-attention (ğŸ”¥ trainable)
        Cross-attention (ğŸ”¥ trainable) - 2 layerë§ˆë‹¤ í•œ ë²ˆ
        Feed forward (ğŸ”¥ trainable)
    """
    def __init__(self, hidden_size=768, num_heads=12, intermediate_size=3072,
                 dropout=0.1, layer_norm_eps=1e-6):
        super().__init__()

        # Self-Attention
        self.self_ln = nn.LayerNorm(hidden_size, eps=layer_norm_eps)
        self.self_attn = nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout, batch_first=True)
        self.self_drop = nn.Dropout(dropout)

        # Cross-Attention (query tokens attend to fMRI) - Queryë§Œ LayerNorm (BLIP-2 ë°©ì‹)
        self.cross_ln = nn.LayerNorm(hidden_size, eps=layer_norm_eps)
        self.cross_attn = nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout, batch_first=True)
        self.cross_drop = nn.Dropout(dropout)

        # Feed Forward
        self.ffn_ln = nn.LayerNorm(hidden_size, eps=layer_norm_eps)
        self.fc1 = nn.Linear(hidden_size, intermediate_size)
        self.fc2 = nn.Linear(intermediate_size, hidden_size)
        self.ffn_drop = nn.Dropout(dropout)
        self.activation = nn.GELU()

    def forward(self, x, fmri_feats, attn_mask=None, do_cross=True, n_q=None, cross_attn_mask=None):
        """
        Input:
            x [B, L, 768] - query tokens (+ optional CLIP hidden states)
                           L = n_q (inference) or n_q + n_t (training with mask)
            fmri_feats [B, 100, 768] - fMRI embeddings
            attn_mask: attention mask for Q-T separation
            do_cross: whether to do cross-attention in this layer
            n_q: number of query tokens (for extracting query part in cross-attention)
            cross_attn_mask: FC prior mask for cross-attention [B, n_q, seq_len]
        Output:
            x [B, L, 768]
        """
        # Self-Attention (with mask if provided)
        residual = x
        x_norm = self.self_ln(x)
        x_sa, _ = self.self_attn(x_norm, x_norm, x_norm,
                                  attn_mask=attn_mask, need_weights=False)
        x = residual + self.self_drop(x_sa)

        # Cross-Attention (query -> fMRI) - ì¡°ê±´ë¶€ ì‹¤í–‰, query ë¶€ë¶„ë§Œ
        if do_cross:
            q = x[:, :n_q, :]  #  Query ë¶€ë¶„ë§Œ cross-attention ì ìš© -> [B, n_q, 768]
            q_res = q
            q_norm = self.cross_ln(q)
            q_ca, _ = self.cross_attn(q_norm, fmri_feats, fmri_feats,
                                     attn_mask=cross_attn_mask,  # FC prior mask ì¶”ê°€
                                     need_weights=False)
            q = q_res + self.cross_drop(q_ca)
            x = torch.cat([q, x[:, n_q:, :]], dim=1)  # Query + ë‚˜ë¨¸ì§€ ë‹¤ì‹œ concat

        # Feed Forward
        residual = x
        x_norm = self.ffn_ln(x)
        x_ffn = self.fc2(self.ffn_drop(self.activation(self.fc1(x_norm))))
        x = residual + self.ffn_drop(x_ffn)

        return x


class ConnectomeQFormer(nn.Module):
    """
    (b) Connectome-Q-Former (initialized from CLIP ViT-L/14)

    ê·¸ë¦¼ ì„¤ëª…:
        [B, 100, 768] (fMRI) + query tokens + CLIP hidden states
        -> Connectome-Q-Former blocks (with attention mask)
        -> [B, 101, 768]

    Query tokens: 101ê°œ (100 ROI + 1 CLS token)
    Weights initialized from openai/clip-vit-base-patch16
    Cross-attention: 2 layerë§ˆë‹¤ í•œ ë²ˆ (BLIP-2 ë°©ì‹)
    Layers: 12 (CLIP ViT-B/16 ê¸°ì¤€)

    ë§ˆìŠ¤í¬ ê¸°ë°˜ Self-attention (models.py ë°©ì‹):
        Query + CLIP hidden statesë¥¼ concatí•˜ì—¬ ì²˜ë¦¬
        ë§ˆìŠ¤í¬ë¡œ Q-T ìƒí˜¸ attend ì°¨ë‹¨ -> ë…ë¦½ì  ì²˜ë¦¬
    """
    def __init__(self, hidden_size=768, num_heads=12, num_layers=12,
                 num_query_tokens=101, dropout=0.1, cross_attention_freq=2,
                 clip_model_name="openai/clip-vit-base-patch16",
                 is_fc=False, subjects=None, fc_base_dir=None):
        super().__init__()

        self.hidden_size = hidden_size
        self.num_query_tokens = num_query_tokens
        self.num_layers = num_layers
        self.cross_attention_freq = cross_attention_freq
        self.is_fc = is_fc
        self.num_heads = num_heads

        # Learnable FC prior scaling parameter (ì´ˆê¸°ê°’ 1.0)
        self.fc_prior_scale = nn.Parameter(torch.ones(1))

        # Cross-attention ì ìš© ì—¬ë¶€ ë¯¸ë¦¬ ê³„ì‚° (0, 2, 4, ... ë²ˆì§¸ layer) - BLIP-2 ë°©ì‹
        self._do_cross_map = [(cross_attention_freq > 0) and (i % cross_attention_freq == 0)
                              for i in range(num_layers)]

        # Learnable query tokens [1, 101, 768] - position embedding ì—†ìŒ
        self.query_tokens = nn.Parameter(torch.randn(1, num_query_tokens, hidden_size))
        nn.init.normal_(self.query_tokens, std=0.02)

        # Connectome-Q-Former blocks (12 layers, CLIP ViT-B/16 ê¸°ì¤€)
        self.blocks = nn.ModuleList([
            ConnectomeQFormerBlock(hidden_size, num_heads, hidden_size * 4, dropout)
            for _ in range(num_layers)
        ])

        # Final LayerNorm
        self.final_ln = nn.LayerNorm(hidden_size)

        # FC prior ì´ˆê¸°í™” (subjectë³„ë¡œ ì €ì¥)
        self.fc_priors = {}  # Dict[subject_name, Tensor[101, 100]]
        if is_fc and subjects is not None and fc_base_dir is not None:
            self._load_fc_priors(subjects, fc_base_dir, num_query_tokens)

        # Initialize from CLIP weights
        self._init_from_clip(clip_model_name)

    def _init_from_clip(self, clip_model_name):
        """CLIP ViT-B/16ì—ì„œ weights ê°€ì ¸ì™€ì„œ ì´ˆê¸°í™” (Self-Attention + FFN)"""
        print(f"Initializing Q-Former from {clip_model_name}...")

        clip_model = CLIPVisionModel.from_pretrained(clip_model_name)
        clip_layers = clip_model.vision_model.encoder.layers

        # CLIP ViT-B/16: 12 layers, hidden_size=768 (ì°¨ì› ì¼ì¹˜!)
        for i, (block, clip_layer) in enumerate(zip(self.blocks, clip_layers)):
            # Self-Attention weights
            # CLIP: layer_norm1 -> self_attn
            block.self_ln.weight.data.copy_(clip_layer.layer_norm1.weight.data)
            block.self_ln.bias.data.copy_(clip_layer.layer_norm1.bias.data)

            # HuggingFace CLIP uses separate q_proj, k_proj, v_proj
            # PyTorch MultiheadAttention uses combined in_proj_weight [3*hidden, hidden]
            # Concatenate q, k, v weights into in_proj_weight
            q_weight = clip_layer.self_attn.q_proj.weight.data
            k_weight = clip_layer.self_attn.k_proj.weight.data
            v_weight = clip_layer.self_attn.v_proj.weight.data
            block.self_attn.in_proj_weight.data.copy_(torch.cat([q_weight, k_weight, v_weight], dim=0))

            q_bias = clip_layer.self_attn.q_proj.bias.data
            k_bias = clip_layer.self_attn.k_proj.bias.data
            v_bias = clip_layer.self_attn.v_proj.bias.data
            block.self_attn.in_proj_bias.data.copy_(torch.cat([q_bias, k_bias, v_bias], dim=0))

            block.self_attn.out_proj.weight.data.copy_(clip_layer.self_attn.out_proj.weight.data)
            block.self_attn.out_proj.bias.data.copy_(clip_layer.self_attn.out_proj.bias.data)

            # FFN weights
            # CLIP: layer_norm2 -> mlp (fc1 -> activation -> fc2)
            block.ffn_ln.weight.data.copy_(clip_layer.layer_norm2.weight.data)
            block.ffn_ln.bias.data.copy_(clip_layer.layer_norm2.bias.data)
            block.fc1.weight.data.copy_(clip_layer.mlp.fc1.weight.data)
            block.fc1.bias.data.copy_(clip_layer.mlp.fc1.bias.data)
            block.fc2.weight.data.copy_(clip_layer.mlp.fc2.weight.data)
            block.fc2.bias.data.copy_(clip_layer.mlp.fc2.bias.data)

            # Cross-Attentionì€ ëœë¤ ì´ˆê¸°í™” ìœ ì§€ (BLIP-2 ë°©ì‹)

        # Final LayerNorm from CLIP post_layernorm
        self.final_ln.weight.data.copy_(clip_model.vision_model.post_layernorm.weight.data)
        self.final_ln.bias.data.copy_(clip_model.vision_model.post_layernorm.bias.data)

        del clip_model
        print(f"âœ… Q-Former initialized from CLIP ViT-B/16 (12 layers, hidden_size=768)")

    def _load_fc_priors(self, subjects, fc_base_dir, num_query_tokens):
        """
        ëª¨ë“  subjectì˜ FC priorë¥¼ ë¡œë“œí•˜ê³  num_headsë§Œí¼ í™•ì¥í•˜ì—¬ ì €ì¥

        Args:
            subjects: list of subject names (e.g., ["sub-01", "sub-02", "sub-05", "sub-07"])
            fc_base_dir: base directory for FC prior files
            num_query_tokens: number of query tokens (101)

        FC prior shape: [100, 100] â†’ Cross-attention mask: [num_heads, 101, 100]
            - [0, :]: CLS token (all zeros, no FC prior)
            - [1:101, :]: 100 ROI queries with FC prior
            - num_heads ì°¨ì› ì¶”ê°€: ëª¨ë“  headì— ë™ì¼í•œ FC prior ì ìš©
        """
        import numpy as np

        seq_len = num_query_tokens - 1  # 100 (CLS ì œì™¸)

        for sub in subjects:
            # FC prior íŒŒì¼ ê²½ë¡œ
            fc_path = f"{fc_base_dir}/{sub}/{sub}_FC_schaefer-100.npy"

            if not os.path.exists(fc_path):
                print(f"âš ï¸  FC prior not found: {fc_path}")
                continue

            # FC prior ë¡œë“œ: [100, 100]
            fc_prior = np.load(fc_path).astype(np.float32)

            assert fc_prior.shape[0] == fc_prior.shape[1] == seq_len, \
                f"FC prior shape mismatch for {sub}: {fc_prior.shape} vs {seq_len}"

            # Z-score ì •ê·œí™” (ì „ì²´): (fc_prior - mean) / std
            # ì „ì²´ 10,000ê°œ ê°’ì— ëŒ€í•´ í•˜ë‚˜ì˜ ë¶„í¬ë¡œ ì •ê·œí™”
            fc_mean = fc_prior.mean()  # scalar
            fc_std = fc_prior.std()    # scalar
            # stdê°€ 0ì´ ì•„ë‹ ë•Œë§Œ ì •ê·œí™”
            if fc_std > 1e-6:
                fc_prior = (fc_prior - fc_mean) / fc_std

            # Cross-attention mask ìƒì„±: [101, 100]
            mask = torch.zeros(num_query_tokens, seq_len, dtype=torch.float32)
            mask[1:, :] = torch.from_numpy(fc_prior)  # [1:101, :] = FC prior (z-scored per row)
            # mask[0, :] = 0 (CLS tokenì€ ì´ë¯¸ 0)

            # num_headsë§Œí¼ í™•ì¥: [num_heads, 101, 100]
            # ëª¨ë“  attention headì— ë™ì¼í•œ FC prior ì ìš©
            mask = mask.unsqueeze(0).repeat(self.num_heads, 1, 1)

            # ì €ì¥ (device ì´ë™ì€ forward ì‹œ ìˆ˜í–‰)
            self.fc_priors[sub] = mask

            print(f"âœ“ FC prior loaded for {sub}: shape={mask.shape}, range=[{fc_prior.min():.4f}, {fc_prior.max():.4f}]")

        print(f"âœ… Total {len(self.fc_priors)} FC priors loaded (pre-expanded for {self.num_heads} heads)")

    def build_mask(self, n_q, n_t, device):
        """
        Attention mask ìƒì„± (models.py ë°©ì‹)

             Q    T
        Q  [â–¡â–¡] [â– â– ]
        T  [â– â– ] [â–¡â–¡]

        â–¡: attend ê°€ëŠ¥ (False)
        â– : attend ë¶ˆê°€ (True)

        -> Që¼ë¦¬ë§Œ attend, Të¼ë¦¬ë§Œ attend (ìƒí˜¸ ì°¨ë‹¨)
        """
        L = n_q + n_t
        mask = torch.zeros(L, L, dtype=torch.bool, device=device)
        mask[:n_q, n_q:] = True  # Q -> T ì°¨ë‹¨
        mask[n_q:, :n_q] = True  # T -> Q ì°¨ë‹¨
        return mask

    def _build_fc_prior_mask(self, subject_names, device, dtype=None):
        """
        Batchì˜ ê° ìƒ˜í”Œì— ëŒ€í•´ í•´ë‹¹ subjectì˜ FC prior maskë¥¼ ìƒì„±

        Args:
            subject_names: list of subject names [B] (e.g., ["sub-01", "sub-02", ...])
            device: torch device
            dtype: torch dtype (for mixed precision compatibility)

        Returns:
            mask: [B*num_heads, 101, 100] - MultiheadAttention í˜•ì‹ì˜ FC prior mask (scaled)
        """
        masks = []
        for sub_name in subject_names:
            if sub_name in self.fc_priors:
                mask = self.fc_priors[sub_name].to(device=device, dtype=dtype if dtype else self.fc_priors[sub_name].dtype)
            else:
                # FC prior ì—†ìœ¼ë©´ zero mask (FC prior ì—†ì´ ë™ì‘)
                mask = torch.zeros(self.num_heads, self.num_query_tokens, self.num_query_tokens - 1,
                                 dtype=dtype if dtype else torch.float32, device=device)
            masks.append(mask)

        # [B*num_heads, 101, 100] í˜•íƒœë¡œ concat
        fc_prior_mask = torch.cat(masks, dim=0)

        # Learnable scaling parameter ì ìš©
        fc_prior_mask = fc_prior_mask * self.fc_prior_scale

        return fc_prior_mask

    
    def forward(self, fmri_emb, clip_hidden=None, use_mask=True, subject_names=None):
        """
        Input:
            fmri_emb [B, 100, 768]
            clip_hidden [B, 257, 768] (optional) - CLIP hidden states for masked self-attention
            use_mask: True=Q-T ìƒí˜¸ ì°¨ë‹¨ (FIRìš©), False=ì „ë²”ìœ„ í—ˆìš© (FIMìš©)
            subject_names: list of subject names [B] (for FC prior)
        Output:
            query_output [B, 101, 768]
        """
        B = fmri_emb.size(0)
        device = fmri_emb.device

        # fMRI features (position embedding ì—†ìŒ)
        fmri_feats = fmri_emb  # [B, 100, 768]

        # Expand query tokens for batch (position embedding ì—†ìŒ)
        query = self.query_tokens.expand(B, -1, -1)  # [B, 101, 768]

        # Concat query + CLIP hidden states (if provided)
        if clip_hidden is not None:
            # [B, 101, 768] + [B, 257, 768] -> [B, 358, 768]
            x = torch.cat([query, clip_hidden], dim=1)
            n_q = self.num_query_tokens
            n_t = clip_hidden.size(1)
            # use_mask=False: ì „ë²”ìœ„ í—ˆìš© (FIMìš©) vs use_mask=True: Q-T ìƒí˜¸ ì°¨ë‹¨ (MSEìš©)
            attn_mask = self.build_mask(n_q, n_t, device) if use_mask else None
        else:
            x = query
            attn_mask = None

        # FC prior mask ìƒì„± (subjectë³„ë¡œ ë™ì  ìƒì„±)
        cross_attn_mask = None
        if self.is_fc and subject_names is not None:
            cross_attn_mask = self._build_fc_prior_mask(subject_names, device, dtype=fmri_feats.dtype)

        # Pass through Connectome-Q-Former blocks
        for i, block in enumerate(self.blocks):
            x = block(x, fmri_feats,
                     attn_mask=attn_mask,
                     do_cross=self._do_cross_map[i],
                     n_q=self.num_query_tokens,
                     cross_attn_mask=cross_attn_mask)

        # Extract query part only (if CLIP hidden was concatenated)
        if clip_hidden is not None:
            query_output = x[:, :self.num_query_tokens, :]  # [B, 101, 768]
        else:
            query_output = x

        # Final LayerNorm
        query_output = self.final_ln(query_output)  # [B, 101, 768]

        return query_output


# ============================================================================
# Low-Level Image Decoder
# ============================================================================

class LowLevelDecoder(nn.Module):
    """
    fMRI ì„ë² ë”©ì—ì„œ Low-level (blurry) ì´ë¯¸ì§€ ìƒì„± (MindEye2 ë°©ì‹)

    input:
        fmri_emb: [B, 100, 768]
    output:
        lowlevel_l1: [B, 4, 28, 28] - VAE latent space (L1 lossìš©, 224x224 ì´ë¯¸ì§€ ê¸°ì¤€)
    """
    def __init__(self, seq_len=100, embed_dim=768):
        super().__init__()

        self.flatten_dim = seq_len * embed_dim  # 100 * 768 = 76800

        # fmri_emb -> [B, 64, 7, 7] feature map (64*7*7 = 3136)
        self.blin1 = nn.Linear(self.flatten_dim, 64 * 7 * 7, bias=True)
        self.bdropout = nn.Dropout(0.3)
        self.bnorm = nn.GroupNorm(1, 64)

        # [B, 64, 7, 7] -> [B, 4, 28, 28] (VAE latent space, 3ê°œ UpBlockìœ¼ë¡œ 4x upsampling)
        self.bupsampler = Decoder(
            in_channels=64,
            out_channels=4,
            up_block_types=["UpDecoderBlock2D", "UpDecoderBlock2D", "UpDecoderBlock2D"],
            block_out_channels=[32, 64, 128],
            layers_per_block=1,
        )

    def forward(self, fmri_emb):
        """
        Input: fmri_emb [B, 100, 768]
        Output:
            lowlevel_l1: [B, 4, 28, 28] - VAE latent (for L1 loss with vae.encode(image))
        """
        B = fmri_emb.size(0)
        x = fmri_emb.view(B, -1)  # [B, 153600]

        # linear -> dropout -> reshape -> groupnorm
        lowlevel = self.blin1(x)  # [B, 64*7*7]
        lowlevel = self.bdropout(lowlevel)
        lowlevel = lowlevel.reshape(B, 64, 7, 7).contiguous()  # [B, 64, 7, 7]
        lowlevel = self.bnorm(lowlevel)

        # L1 lossìš©: VAE latent spaceë¡œ upsampling
        lowlevel_l1 = self.bupsampler(lowlevel)  # [B, 4, 28, 28]

        return lowlevel_l1


# ============================================================================
# Output Projection (Linear layer + L2 norm)
# ============================================================================

class OutputProjection(nn.Module):
    """
    Q-Former ì¶œë ¥ì„ CLIP spaceë¡œ projection (Transpose ë°©ì‹)

    ê·¸ë¦¼ ì„¤ëª…:
        Q-Former output [B, 101, 768]
        -> Transpose -> [B, 768, 101]
        -> Linear(101, 257) -> [B, 768, 257]
        -> Transpose -> [B, 257, 768]
        -> L2 norm -> [B, 257, 768]

    íŒŒë¼ë¯¸í„° ìˆ˜: 101 * 257 = 25,957 (Flatten ë°©ì‹ ëŒ€ë¹„ ~1,200,000ë°° ì ìŒ)
    """
    def __init__(self, input_tokens=101, output_tokens=257, hidden_size=768):
        super().__init__()
        self.input_tokens = input_tokens
        self.output_tokens = output_tokens
        self.hidden_size = hidden_size

        # [B, 768, 101] -> [B, 768, 257] (ê° dimensionë³„ ë…ë¦½ projection)
        self.proj = nn.Linear(input_tokens, output_tokens)

    def forward(self, x):
        """
        Input: x [B, 101, 768]
        Output: x [B, 257, 768] (L2 normalized)
        """
        x = x.transpose(1, 2)  # [B, 768, 101]
        x = self.proj(x)       # [B, 768, 257]
        x = x.transpose(1, 2)  # [B, 257, 768]
        x = F.normalize(x, dim=-1)  # L2 norm
        return x


# ============================================================================
# Loss Functions
# ============================================================================

class FIRLoss(nn.Module):
    """
    FIR Loss (fMRI-Image Reconstruction): fMRI embedding vs CLIP embedding

    ê·¸ë¦¼ ì„¤ëª…:
        Linear layer [B, 257, 768] <------ FIR Loss ------> Linear layer [B, 257, 768]
        (from Q-Former)                                     (from CLIP)

    L1 Loss ì‚¬ìš©: MSEë³´ë‹¤ gradientê°€ ì•ˆì •ì  (í° ì˜¤ì°¨ì—ì„œë„ gradientê°€ ì¼ì •)
    """
    def __init__(self):
        super().__init__()
        self.l1 = nn.L1Loss()

    def forward(self, fmri_emb, clip_emb):
        """
        Input:
            fmri_emb [B, 257, 768] - Q-Former output (L2 normalized)
            clip_emb [B, 257, 768] - CLIP output (L2 normalized)
        Output: scalar loss
        """
        return self.l1(fmri_emb, clip_emb)


class CrossEntropyLoss(nn.Module):
    """FIM Loss: BCE (matching)"""
    def forward(self, logits, labels):
        return F.binary_cross_entropy_with_logits(logits.view(-1), labels.view(-1).float())


# ============================================================================
# Complete Model
# ============================================================================

class ConnecToMind2(nn.Module):
    """
    ConnecToMind2 - Model2 (New Architecture)

    Architecture:
        fMRI [B, 100, input_dim]
            -> (a) Region-level embedding -> [B, 100, 768]
            -> (b) Connectome-Q-Former -> [B, 101, 768]
            -> Linear layer -> [B, 257, 768]
            -> L2 norm -> [B, 257, 768]

        Image [B, 3, 224, 224]
            -> CLIP ViT-L/14 -> [B, 257, 1024]
            -> Linear layer + L2 norm -> [B, 257, 768]

    Loss = FIR Loss (fMRI embedding vs CLIP embedding, MSE)
         + Cross Entropy Loss (CLS token contrastive)
         + Low-level Loss (VAE L1)

    Output (training):
        fmri_proj: [B, 257, 768] - Q-Former output (Linear + L2 norm)
        clip_proj: [B, 257, 768] - CLIP output (Linear + L2 norm)
        fmri_cls: [B, 768] - fMRI CLS token
        clip_cls: [B, 768] - CLIP CLS token
        lowlevel_l1: [B, 4, 28, 28] - VAE latent (for L1 loss)
        loss_fir: FIR loss
        loss_cls: Cross entropy loss

    Versatile Diffusion inputs:
        - image_embeds: fmri_proj [B, 257, 768]
        - image: VAE decode(lowlevel_l1) for image condition
    """
    def __init__(self, seq_len=100, input_dim=3291, embed_dim=768,
                 num_qformer_layers=12, num_query_tokens=101,
                 is_fc=False, subjects=None, fc_base_dir=None):
        super().__init__()

        self.seq_len = seq_len
        self.embed_dim = embed_dim
        self.num_query_tokens = num_query_tokens

        # 1. CLIP Image Encoder (ViT-L/14, frozen)
        self.clip_encoder = CLIPImageEncoder(freeze=True)

        # 2. Region-level Embedding
        self.region_embedding = RegionLevelEmbedding(
            seq_len=seq_len,
            input_dim=input_dim,
            embed_dim=embed_dim
        )

        # 3. Connectome-Q-Former (initialized from CLIP, with FC prior)
        self.connectome_qformer = ConnectomeQFormer(
            hidden_size=embed_dim,
            num_heads=12,
            num_layers=num_qformer_layers,
            num_query_tokens=num_query_tokens,
            dropout=0.1,
            is_fc=is_fc,
            subjects=subjects,
            fc_base_dir=fc_base_dir
        )

        # 4. Output Projection: [B, 101, 768] -> [B, 257, 768]
        self.output_proj = OutputProjection(
            input_tokens=num_query_tokens,
            output_tokens=257,
            hidden_size=embed_dim
        )

        # 5. Low-Level Decoder
        self.low_level_decoder = LowLevelDecoder(seq_len=seq_len, embed_dim=embed_dim)

        # 6. FIM classifier: CLS token [768] -> logit [1]
        self.fim_classifier = nn.Linear(embed_dim, 1)

        # 7. Loss functions
        self.fir_loss_fn = FIRLoss()
        self.fim_loss_fn = CrossEntropyLoss()  # BCE loss

    def forward(self, fmri, images, device, subject_names=None):
        """
        Training forward (Q-Former 2ë²ˆ í˜¸ì¶œ: FIRìš© ë§ˆìŠ¤í¬ O, FIMìš© ë§ˆìŠ¤í¬ X)

        FIR: Q-T ìƒí˜¸ ì°¨ë‹¨ ë§ˆìŠ¤í¬ ì‚¬ìš© (Queryê°€ CLIPì„ ì§ì ‘ ë³´ë©´ cheating)
        FIM: ë§ˆìŠ¤í¬ ì—†ìŒ (Queryê°€ CLIPì„ ë³´ê³  matching íŒë‹¨)

        Input:
            fmri [B, 100, input_dim]
            images [B, 3, 224, 224]
            device: torch device
            subject_names: list of subject names [B] (for FC prior)

        Output: dict
            fmri_proj: [B, 257, 768] - for FIR loss
            clip_proj: [B, 257, 768] - for FIR loss
            lowlevel_l1: [B, 4, 28, 28] - for L1 loss with vae.encode(image)
            loss_fir: scalar
            loss_fim: scalar
        """
        B = fmri.size(0)

        # === Image path (CLIP) ===
        clip_proj = self.clip_encoder(images)  # [B, 257, 768]

        # === fMRI path ===
        # (a) Region-level embedding
        fmri_emb = self.region_embedding(fmri)  # [B, 100, 768]

        # === FIR Branch (ë§ˆìŠ¤í¬ O: Q-T ìƒí˜¸ ì°¨ë‹¨) ===
        qformer_out_fir = self.connectome_qformer(fmri_emb, clip_proj, use_mask=True, subject_names=subject_names)  # [B, 101, 768]
        fmri_proj = self.output_proj(qformer_out_fir)  # [B, 257, 768]

        # === FIM Branch (ë§ˆìŠ¤í¬ X: ì „ë²”ìœ„ í—ˆìš©) ===
        perm = torch.randperm(B, device=device)
        fmri_emb_2b = torch.cat([fmri_emb, fmri_emb], dim=0)  # [2B, 100, 768]
        clip_proj_2b = torch.cat([clip_proj, clip_proj[perm]], dim=0)  # [2B, 257, 768]
        fim_labels = torch.cat([torch.ones(B, device=device), torch.zeros(B, device=device)])  # [2B]

        # FIM Branchìš© subject_namesë„ 2ë°°ë¡œ ë³µì œ
        subject_names_2b = subject_names + subject_names if subject_names is not None else None

        qformer_out_fim = self.connectome_qformer(fmri_emb_2b, clip_proj_2b, use_mask=False, subject_names=subject_names_2b)  # [2B, 101, 768]

        # Low-level decoder (ì›ë³¸ fmri_embë§Œ ì‚¬ìš©)
        lowlevel_l1 = self.low_level_decoder(fmri_emb)  # [B, 4, 28, 28]

        # === Compute FIR loss ===
        loss_fir = self.fir_loss_fn(fmri_proj, clip_proj)

        # === FIM Loss ===
        query_cls_2b = qformer_out_fim[:, 0, :]  # [2B, 768]
        fim_logits = self.fim_classifier(query_cls_2b).squeeze(-1)  # [2B]
        loss_fim = self.fim_loss_fn(fim_logits, fim_labels)

        return {
            "fmri_proj": fmri_proj,          # [B, 257, 768]
            "clip_proj": clip_proj,          # [B, 257, 768]
            "lowlevel_l1": lowlevel_l1,      # [B, 4, 28, 28]
            "loss_fir": loss_fir,
            "loss_fim": loss_fim,
        }

    def inference(self, fmri, subject_names=None):
        """
        Inference (ì´ë¯¸ì§€ ì—†ì´)

        Input:
            fmri [B, 100, input_dim]
            subject_names: list of subject names [B] (for FC prior)
        Output:
            fmri_proj: [B, 257, 768] - Versatile Diffusionì˜ image_embedsë¡œ ì‚¬ìš©
            lowlevel_l1: [B, 4, 28, 28] - VAE decodeí•˜ë©´ blurry image
        """
        # (a) Region-level embedding
        fmri_emb = self.region_embedding(fmri)  # [B, 100, 768]

        # (b) Connectome-Q-Former
        qformer_out = self.connectome_qformer(fmri_emb, subject_names=subject_names)  # [B, 257, 768]

        # Linear layer + L2 norm
        fmri_proj = self.output_proj(qformer_out)  # [B, 257, 768]

        # Low-level decoder
        lowlevel_l1 = self.low_level_decoder(fmri_emb)  # [B, 4, 28, 28]

        return {
            "fmri_proj": fmri_proj,      # [B, 257, 768]
            "lowlevel_l1": lowlevel_l1,  # [B, 4, 28, 28]
        }


# ============================================================================
# Model Factory
# ============================================================================

def get_model(args):
    """
    ëª¨ë¸ ìƒì„±

    args í•„ìš” ì†ì„±:
        - seq_len, input_dim, embed_dim, num_qformer_layers, num_query_tokens
        - cache_dir: pretrained model cache ê²½ë¡œ

    returns:
        connectomind2: ë©”ì¸ ëª¨ë¸
        versatile_diffusion: Versatile Diffusion pipeline
        vae: VAE encoder (for L1 loss)
        l1: L1 loss function
    """
    # ìºì‹œ ê²½ë¡œ (ë¡œì»¬ ìš°ì„ , ì—†ìœ¼ë©´ ì˜¨ë¼ì¸ì—ì„œ ë‹¤ìš´ë¡œë“œ)
    cache_dir = args.cache_dir

    # FC prior base directory (is_fcê°€ Trueì¸ ê²½ìš°ì—ë§Œ ì‚¬ìš©)
    fc_base_dir = None
    if args.is_fc:
        fc_base_dir = f"{args.root_dir}/{args.fmri_dir}/{args.fmri_detail_dir}"

    # ë©”ì¸ ëª¨ë¸
    connectomind2 = ConnecToMind2(
        seq_len=args.seq_len,
        input_dim=args.input_dim,
        embed_dim=args.embed_dim,
        num_qformer_layers=args.num_qformer_layers,
        num_query_tokens=args.num_query_tokens,
        is_fc=args.is_fc,
        subjects=args.subjects,
        fc_base_dir=fc_base_dir,
    )

    # High-level reconstruction ìš©ë„ -> Versatile Diffusion pipeline
    # ë¡œì»¬ì— ìˆìœ¼ë©´ ë¡œì»¬ì—ì„œ, ì—†ìœ¼ë©´ ì˜¨ë¼ì¸ì—ì„œ ë‹¤ìš´ë¡œë“œ í›„ cache_dirì— ì €ì¥
    print("Loading Versatile Diffusion pipeline...")
    try:
        versatile_diffusion = DiffusionPipeline.from_pretrained(
            "shi-labs/versatile-diffusion",
            torch_dtype=torch.float16,
            cache_dir=cache_dir,
            local_files_only=True  # ë¡œì»¬ ìš°ì„  ì‹œë„
        )
        print("âœ… Versatile Diffusion loaded (from local cache)")
    except Exception:
        print("  ë¡œì»¬ ìºì‹œ ì—†ìŒ, ì˜¨ë¼ì¸ì—ì„œ ë‹¤ìš´ë¡œë“œ...")
        try:
            versatile_diffusion = DiffusionPipeline.from_pretrained(
                "shi-labs/versatile-diffusion",
                torch_dtype=torch.float16,
                cache_dir=cache_dir
            )
            print("âœ… Versatile Diffusion downloaded and cached")
        except Exception as e:
            print(f"[!] Versatile Diffusion ë¡œë”© ì‹¤íŒ¨: {e}")
            versatile_diffusion = None

    # Low-level reconstruction ìš©ë„ -> VAE (for L1 loss - low-level)
    print("Loading VAE...")
    try:
        sd_pipe = DiffusionPipeline.from_pretrained(
            "lambdalabs/sd-image-variations-diffusers",
            cache_dir=cache_dir,
            local_files_only=True  # ë¡œì»¬ ìš°ì„  ì‹œë„
        )
        print("âœ… VAE loaded (from local cache)")
    except Exception:
        print("  ë¡œì»¬ ìºì‹œ ì—†ìŒ, ì˜¨ë¼ì¸ì—ì„œ ë‹¤ìš´ë¡œë“œ...")
        sd_pipe = DiffusionPipeline.from_pretrained(
            "lambdalabs/sd-image-variations-diffusers",
            cache_dir=cache_dir
        )
        print("âœ… VAE downloaded and cached")
    vae = sd_pipe.vae
    vae.eval().requires_grad_(False)

    # L1 loss
    l1 = nn.L1Loss()

    return {
        "connectomind2": connectomind2,
        "versatile_diffusion": versatile_diffusion,
        "vae": vae,
        "l1": l1,
    }
