import re, os
import gc
import time
from tqdm import tqdm
import wandb

import numpy as np
from scipy import stats
from accelerate import Accelerator
import torch
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.set_float32_matmul_precision("high")
torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=True)
import torch.nn as nn
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data.distributed import DistributedSampler
from torch.profiler import profile, record_function, ProfilerActivity
from torchvision import transforms

from utils import img_augment_high, mixup, mixco_nce_loss, cosine_anneal, soft_clip_loss, topk, batchwise_cosine_similarity, log_gradient_norms, check_nan_and_log, reconstruction, get_unique_path, img_augment_low, soft_cont_loss, unclip_recon, sdxl_recon, save_gt_vs_recon_images_extended

def pre_train(args, subj_names, train_data, models, optimizer, lr_scheduler):

    device = args.device
    num_epochs = args.num_epochs
    mixup_pct = args.mixup_pct
    prior_loss_coefficient = args.prior_loss_coefficient
    nce_loss_coefficient = args.nce_loss_coefficient
    lowlevel_loss_coefficient = args.lowlevel_loss_coefficient

    scaler = GradScaler(enabled=False) # autocast scaler ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    subj_names = subj_names


    # model ì •ì˜
    clip_extractor = models["clip"]
    mindeye2 = models["mindeye2"]
    vae = models["vae"]
    cnx = models["cnx"]
    l1 = models["l1"]
    optimizer = optimizer
    lr_scheduler = lr_scheduler

    # log list
    losses, lrs = [], []
    progress_bar = tqdm(range(0, num_epochs), ncols=50)
    for epoch in progress_bar:
        mindeye2.train()

        # ê¸°ë³¸ log
        loss_prior_sum = 0.0  # prior lossì˜ ëˆ„ì í•© -> í‰ê·  êµ¬í•  ë•Œ ì“°ì„
        loss_nce_sum = 0.0 # Negative Contrastive Estimation lossì˜ ëˆ„ì í•© -> í‰ê·  êµ¬í•  ë•Œ ì“°ì„
        loss_blurry_sum = 0.0 # l1 lossì˜ ëˆ„ì í•© -> í‰ê·  êµ¬í•  ë•Œ ì“°ì„
        loss_blurry_cont_sum = 0.0 # contlossì˜ ëˆ„ì í•© -> í‰ê·  êµ¬í•  ë•Œ ì“°ì„

        
        for index, batch in enumerate(train_data): # enumerate: indexì™€ ê°’ì„ ê°™ì´ ë°˜í™˜
            # global step ê³„ì‚°
            global_step = epoch * len(train_data) + index

            optimizer.zero_grad()

            fmri_list, image_list = {}, {}
            perm_list, betas_list, select_list = {}, {}, {}
            use_mix = epoch < int(mixup_pct * num_epochs)

            for subj in subj_names:
                fmri_vol, img = batch[subj]          
                fmri_vol = fmri_vol.to(device, non_blocking=True)
                img = img.to(device,  non_blocking=True)
                
                # epochì˜ 1/3 ì§€ì  ê¹Œì§€ë§Œ mixup ì‚¬ìš©
                if use_mix:
                    fmri_vol, perm, betas, select = mixup(fmri_vol)
                    perm_list[subj] = perm
                    betas_list[subj] = betas
                    select_list[subj] = select

                fmri_list[subj] = fmri_vol
                image_list[subj] = img

            image = torch.cat([image_list[subj] for subj in subj_names], dim=0)      
            if use_mix:
                perm   = torch.cat([perm_list[subj] for subj in subj_names], dim=0)
                betas  = torch.cat([betas_list[subj] for subj in subj_names], dim=0)
                select = torch.cat([select_list[subj] for subj in subj_names], dim=0)    

            # ê³ í•´ìƒë„ ì´ë¯¸ì§€ ì¦ê°•
            image = img_augment_high(image)

            with autocast(dtype=torch.bfloat16):
                #### forward ê³„ì‚° + loss ê³„ì‚° ####
                with torch.no_grad():
                    # target ì •ì˜
                    clip_target = clip_extractor(image)[0]

                # Shared-subject latent space(each -> 4096)
                voxel_ridge_list = [mindeye2.ridge(fmri_list[subj], i) for i, subj in enumerate(subj_names)]
                voxel_ridge = torch.cat(voxel_ridge_list, dim=0)

                # Residual MLP backbone 
                voxel_backbone, voxel_retrieval, voxel_lowlevels = mindeye2.backbone(voxel_ridge)

                # forward(Diffusion prior) -> prior loss
                loss_prior, _ = mindeye2.diffusion_prior(text_embed=voxel_backbone, image_embed=clip_target)
                
                # forward(retrieval submodule) -> contrstive loss(mixco_nce_loss + soft_clip_loss)
                clip_voxels_norm = nn.functional.normalize(voxel_retrieval.flatten(1), dim=-1).float()
                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1).float()
                # mixco_nce_loss(1/3) + soft_loss_temps(2/3)
                if epoch < int(mixup_pct * num_epochs):
                    nce_loss = mixco_nce_loss(
                        clip_voxels_norm,
                        clip_target_norm,
                        temp=.006, 
                        perm=perm, betas=betas, select=select)
                else:
                    soft_loss_temps = cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))
                    epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]
                    nce_loss = soft_clip_loss(
                        clip_voxels_norm,
                        clip_target_norm,
                        temp=epoch_temp)
                
                # forward(low-level submodule) -> l1_loss + cnx_loss
                image_enc_pred, transformer_feats = voxel_lowlevels

                with torch.no_grad():
                    image_enc = vae.encode(2*image-1).latent_dist.mode() * 0.18215
                loss_blurry = l1(image_enc_pred, image_enc)

                with torch.no_grad():
                    mean = torch.tensor([0.485, 0.456, 0.406]).to(device).reshape(1,3,1,1) # imagenetì˜ mean
                    std = torch.tensor([0.228, 0.224, 0.225]).to(device).reshape(1,3,1,1) # imagenetì˜ std
                    image_norm = (image - mean)/std
                    image_aug = (img_augment_low(image) - mean)/std
                    _, cnx_embeds = cnx(image_norm)
                    _, cnx_aug_embeds = cnx(image_aug)
                cont_loss = soft_cont_loss(
                    nn.functional.normalize(transformer_feats.reshape(-1, transformer_feats.shape[-1]), dim=-1),
                    nn.functional.normalize(cnx_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),
                    nn.functional.normalize(cnx_aug_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),
                    temp=0.2
                )

                # ìµœì¢… loss ì •ì˜
                loss = (prior_loss_coefficient * loss_prior) + (nce_loss_coefficient * nce_loss) + (lowlevel_loss_coefficient * (loss_blurry + 0.1*cont_loss)) 

                # NaN ì²´í¬ + ë„˜ê¹€
                if check_nan_and_log(global_step=index, fmri_vol=fmri_vol, voxel_backbone=voxel_backbone, voxel_retrieval=voxel_retrieval, voxel_lowlevels=voxel_lowlevels, loss=loss):
                    continue

            #### backward ê³„ì‚° + update ####
            # gradient ê³„ì‚° - ampì‚¬ìš©
            scaler.scale(loss).backward() # ampì‚¬ìš©

            # optimizer update - ampì‚¬ìš©
            scaler.step(optimizer) # ampì‚¬ìš©
            scaler.update() # ampì‚¬ìš©

            # torch.cuda.empty_cache() # gpu ë©”ëª¨ë¦¬ cacheì‚­ì œ
            # gc.collect() # # gpu ë©”ëª¨ë¦¬ ì•ˆ ì“°ëŠ”ê±° ì‚­ì œ

            # learning rate schedule update
            lr_scheduler.step()

            #### log ####
            # loss, lr ë‹´ì•„ë‘ê¸°
            losses.append(loss.item())
            lrs.append(optimizer.param_groups[0]['lr'])

            # loss ëˆ„ì í•©
            loss_prior_sum += loss_prior.item()
            loss_nce_sum += nce_loss.item()
            loss_blurry_sum += loss_blurry.item()
            loss_blurry_cont_sum += cont_loss.item()

            logs = {
                # ê¸°ë³¸ í•™ìŠµ ìƒíƒœ
                "epoch": epoch,
                "train/num_steps": index + 1,  # í˜„ì¬ iteration
                "train/lr": lrs[-1],
                "train/global_step": global_step,
                "train/epoch": epoch,
                "train/loss": losses[-1],
                "train/loss_nce": nce_loss.item(),
                "train/loss_prior": loss_prior.item(),
                "train/loss_blurry": loss_blurry.item(),
                "train/loss_cont": cont_loss.item(),

                # ë””ë²„ê·¸: fmri_vol
                "debug/fmri_nan": float(torch.isnan(fmri_vol).any().item()),
                "debug/fmri_min": fmri_vol.min().item(),
                "debug/fmri_max": fmri_vol.max().item(),

                # ë””ë²„ê·¸: voxel_backbone
                "debug/voxel_backbone_nan": float(torch.isnan(voxel_backbone).any().item()),
                "debug/voxel_backbone_min": voxel_backbone.min().item(),
                "debug/voxel_backbone_max": voxel_backbone.max().item(),

                # ë””ë²„ê·¸: voxel_retrieval
                "debug/voxel_retrieval_nan": float(torch.isnan(voxel_retrieval).any().item()),
                "debug/voxel_retrieval_min": voxel_retrieval.min().item(),
                "debug/voxel_retrieval_max": voxel_retrieval.max().item(),

                # ë””ë²„ê·¸: voxel_lowlevels[0] (image_enc_pred)
                "debug/voxel_lowlevels_pred_nan": float(torch.isnan(voxel_lowlevels[0]).any().item()),
                "debug/voxel_lowlevels_pred_min": voxel_lowlevels[0].min().item(),
                "debug/voxel_lowlevels_pred_max": voxel_lowlevels[0].max().item(),

                # ë””ë²„ê·¸: loss ê°’ NaN ì—¬ë¶€
                "debug/loss_nan": float(torch.isnan(loss).item()),
            }
            progress_bar.set_postfix(**logs) # cliì— ì‹œê°í™”
            wandb.log(logs, step=global_step) # wandbì— ì‹œê°í™”

        if epoch >= 140 and epoch % 5 == 0:
        # if epoch %/10 == 0:
            save_path = os.path.join(args.root_dir, args.code_dir, args.output_dir, "mindeye2_metric", f"mindeye2_pretrain_{epoch}_{args.experiment_name}.pt")
            save_path = get_unique_path(save_path)
            os.makedirs(os.path.dirname(save_path), exist_ok=True)  # ê²½ë¡œ ì—†ìœ¼ë©´ ìƒì„±
            torch.save(mindeye2.state_dict(), save_path)

        torch.cuda.empty_cache() # gpu ë©”ëª¨ë¦¬ cacheì‚­ì œ
        gc.collect() # # gpu ë©”ëª¨ë¦¬ ì•ˆ ì“°ëŠ”ê±° ì‚­ì œ

def pre_train_continous(args, subj_names, train_data, models, optimizer, lr_scheduler):

    device = args.device
    num_epochs = args.num_epochs
    mixup_pct = args.mixup_pct
    prior_loss_coefficient = args.prior_loss_coefficient
    nce_loss_coefficient = args.nce_loss_coefficient
    lowlevel_loss_coefficient = args.lowlevel_loss_coefficient

    scaler = GradScaler(enabled=False) # autocast scaler ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    subj_names = subj_names


    # model ì •ì˜
    clip_extractor = models["clip"]
    mindeye2 = models["mindeye2"]
    vae = models["vae"]
    cnx = models["cnx"]
    l1 = models["l1"]
    optimizer = optimizer
    lr_scheduler = lr_scheduler

    ckpt_path = os.path.join(args.root_dir, args.code_dir, args.output_dir, "mindeye2_metric", "mindeye2_pretrain_170_1257.pt")
    checkpoint = torch.load(ckpt_path, map_location="cpu")
    mindeye2.load_state_dict(checkpoint, strict=False)

    # log list
    losses, lrs = [], []
    progress_bar = tqdm(range(0, num_epochs), ncols=50)
    for epoch in progress_bar:
        mindeye2.train()

        # ê¸°ë³¸ log
        loss_prior_sum = 0.0  # prior lossì˜ ëˆ„ì í•© -> í‰ê·  êµ¬í•  ë•Œ ì“°ì„
        loss_nce_sum = 0.0 # Negative Contrastive Estimation lossì˜ ëˆ„ì í•© -> í‰ê·  êµ¬í•  ë•Œ ì“°ì„
        loss_blurry_sum = 0.0 # l1 lossì˜ ëˆ„ì í•© -> í‰ê·  êµ¬í•  ë•Œ ì“°ì„
        loss_blurry_cont_sum = 0.0 # contlossì˜ ëˆ„ì í•© -> í‰ê·  êµ¬í•  ë•Œ ì“°ì„

        
        for index, batch in enumerate(train_data): # enumerate: indexì™€ ê°’ì„ ê°™ì´ ë°˜í™˜
            # global step ê³„ì‚°
            global_step = epoch * len(train_data) + index

            optimizer.zero_grad()

            fmri_list, image_list = {}, {}
            perm_list, betas_list, select_list = {}, {}, {}
            use_mix = epoch < int(mixup_pct * num_epochs)

            for subj in subj_names:
                fmri_vol, img = batch[subj]          
                fmri_vol = fmri_vol.to(device, non_blocking=True)
                img = img.to(device,  non_blocking=True)
                
                # epochì˜ 1/3 ì§€ì  ê¹Œì§€ë§Œ mixup ì‚¬ìš©
                if use_mix:
                    fmri_vol, perm, betas, select = mixup(fmri_vol)
                    perm_list[subj] = perm
                    betas_list[subj] = betas
                    select_list[subj] = select

                fmri_list[subj] = fmri_vol
                image_list[subj] = img

            image = torch.cat([image_list[subj] for subj in subj_names], dim=0)      
            if use_mix:
                perm   = torch.cat([perm_list[subj] for subj in subj_names], dim=0)
                betas  = torch.cat([betas_list[subj] for subj in subj_names], dim=0)
                select = torch.cat([select_list[subj] for subj in subj_names], dim=0)    

            # ê³ í•´ìƒë„ ì´ë¯¸ì§€ ì¦ê°•
            image = img_augment_high(image)

            with autocast(dtype=torch.bfloat16):
                #### forward ê³„ì‚° + loss ê³„ì‚° ####
                with torch.no_grad():
                    # target ì •ì˜
                    clip_target = clip_extractor(image)[0]

                # Shared-subject latent space(each -> 4096)
                voxel_ridge_list = [mindeye2.ridge(fmri_list[subj], i) for i, subj in enumerate(subj_names)]
                voxel_ridge = torch.cat(voxel_ridge_list, dim=0)

                # Residual MLP backbone 
                voxel_backbone, voxel_retrieval, voxel_lowlevels = mindeye2.backbone(voxel_ridge)

                # forward(Diffusion prior) -> prior loss
                loss_prior, _ = mindeye2.diffusion_prior(text_embed=voxel_backbone, image_embed=clip_target)
                
                # forward(retrieval submodule) -> contrstive loss(mixco_nce_loss + soft_clip_loss)
                clip_voxels_norm = nn.functional.normalize(voxel_retrieval.flatten(1), dim=-1).float()
                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1).float()
                # mixco_nce_loss(1/3) + soft_loss_temps(2/3)
                if epoch < int(mixup_pct * num_epochs):
                    nce_loss = mixco_nce_loss(
                        clip_voxels_norm,
                        clip_target_norm,
                        temp=.006, 
                        perm=perm, betas=betas, select=select)
                else:
                    soft_loss_temps = cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))
                    epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]
                    nce_loss = soft_clip_loss(
                        clip_voxels_norm,
                        clip_target_norm,
                        temp=epoch_temp)
                
                # forward(low-level submodule) -> l1_loss + cnx_loss
                image_enc_pred, transformer_feats = voxel_lowlevels

                with torch.no_grad():
                    image_enc = vae.encode(2*image-1).latent_dist.mode() * 0.18215
                loss_blurry = l1(image_enc_pred, image_enc)

                with torch.no_grad():
                    mean = torch.tensor([0.485, 0.456, 0.406]).to(device).reshape(1,3,1,1) # imagenetì˜ mean
                    std = torch.tensor([0.228, 0.224, 0.225]).to(device).reshape(1,3,1,1) # imagenetì˜ std
                    image_norm = (image - mean)/std
                    image_aug = (img_augment_low(image) - mean)/std
                    _, cnx_embeds = cnx(image_norm)
                    _, cnx_aug_embeds = cnx(image_aug)
                cont_loss = soft_cont_loss(
                    nn.functional.normalize(transformer_feats.reshape(-1, transformer_feats.shape[-1]), dim=-1),
                    nn.functional.normalize(cnx_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),
                    nn.functional.normalize(cnx_aug_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),
                    temp=0.2
                )

                # ìµœì¢… loss ì •ì˜
                loss = (prior_loss_coefficient * loss_prior) + (nce_loss_coefficient * nce_loss) + (lowlevel_loss_coefficient * (loss_blurry + 0.1*cont_loss)) 

                # NaN ì²´í¬ + ë„˜ê¹€
                if check_nan_and_log(global_step=index, fmri_vol=fmri_vol, voxel_backbone=voxel_backbone, voxel_retrieval=voxel_retrieval, voxel_lowlevels=voxel_lowlevels, loss=loss):
                    continue

            #### backward ê³„ì‚° + update ####
            # gradient ê³„ì‚° - ampì‚¬ìš©
            scaler.scale(loss).backward() # ampì‚¬ìš©

            # optimizer update - ampì‚¬ìš©
            scaler.step(optimizer) # ampì‚¬ìš©
            scaler.update() # ampì‚¬ìš©

            # torch.cuda.empty_cache() # gpu ë©”ëª¨ë¦¬ cacheì‚­ì œ
            # gc.collect() # # gpu ë©”ëª¨ë¦¬ ì•ˆ ì“°ëŠ”ê±° ì‚­ì œ

            # learning rate schedule update
            lr_scheduler.step()

            #### log ####
            # loss, lr ë‹´ì•„ë‘ê¸°
            losses.append(loss.item())
            lrs.append(optimizer.param_groups[0]['lr'])

            # loss ëˆ„ì í•©
            loss_prior_sum += loss_prior.item()
            loss_nce_sum += nce_loss.item()
            loss_blurry_sum += loss_blurry.item()
            loss_blurry_cont_sum += cont_loss.item()

            logs = {
                # ê¸°ë³¸ í•™ìŠµ ìƒíƒœ
                "epoch": epoch,
                "train/num_steps": index + 1,  # í˜„ì¬ iteration
                "train/lr": lrs[-1],
                "train/global_step": global_step,
                "train/epoch": epoch,
                "train/loss": losses[-1],
                "train/loss_nce": nce_loss.item(),
                "train/loss_prior": loss_prior.item(),
                "train/loss_blurry": loss_blurry.item(),
                "train/loss_cont": cont_loss.item(),

                # ë””ë²„ê·¸: fmri_vol
                "debug/fmri_nan": float(torch.isnan(fmri_vol).any().item()),
                "debug/fmri_min": fmri_vol.min().item(),
                "debug/fmri_max": fmri_vol.max().item(),

                # ë””ë²„ê·¸: voxel_backbone
                "debug/voxel_backbone_nan": float(torch.isnan(voxel_backbone).any().item()),
                "debug/voxel_backbone_min": voxel_backbone.min().item(),
                "debug/voxel_backbone_max": voxel_backbone.max().item(),

                # ë””ë²„ê·¸: voxel_retrieval
                "debug/voxel_retrieval_nan": float(torch.isnan(voxel_retrieval).any().item()),
                "debug/voxel_retrieval_min": voxel_retrieval.min().item(),
                "debug/voxel_retrieval_max": voxel_retrieval.max().item(),

                # ë””ë²„ê·¸: voxel_lowlevels[0] (image_enc_pred)
                "debug/voxel_lowlevels_pred_nan": float(torch.isnan(voxel_lowlevels[0]).any().item()),
                "debug/voxel_lowlevels_pred_min": voxel_lowlevels[0].min().item(),
                "debug/voxel_lowlevels_pred_max": voxel_lowlevels[0].max().item(),

                # ë””ë²„ê·¸: loss ê°’ NaN ì—¬ë¶€
                "debug/loss_nan": float(torch.isnan(loss).item()),
            }
            progress_bar.set_postfix(**logs) # cliì— ì‹œê°í™”
            wandb.log(logs, step=global_step) # wandbì— ì‹œê°í™”

        if epoch >= 130 and epoch % 5 == 0:
        # if epoch %/10 == 0:
            save_path = os.path.join(args.root_dir, args.code_dir, args.output_dir, "mindeye2_metric", f"mindeye2_pretrain_continous_{epoch}_{args.experiment_name}.pt")
            save_path = get_unique_path(save_path)
            os.makedirs(os.path.dirname(save_path), exist_ok=True)  # ê²½ë¡œ ì—†ìœ¼ë©´ ìƒì„±
            torch.save(mindeye2.state_dict(), save_path)

        torch.cuda.empty_cache() # gpu ë©”ëª¨ë¦¬ cacheì‚­ì œ
        gc.collect() # # gpu ë©”ëª¨ë¦¬ ì•ˆ ì“°ëŠ”ê±° ì‚­ì œ



def fine_tunning_train(args, subj_names, train_data, models, optimizer, lr_scheduler):

    # train argument
    device = args.device
    experiment_name = args.experiment_name
    num_epochs = args.num_epochs
    mixup_pct = args.mixup_pct
    prior_loss_coefficient = args.prior_loss_coefficient
    nce_loss_coefficient = args.nce_loss_coefficient
    lowlevel_loss_coefficient = args.lowlevel_loss_coefficient
    subj_names = subj_names

    # test argument
    seed = args.seed

    scaler = GradScaler(enabled=False) # autocast scaler ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    subj_names = subj_names

    # model ì •ì˜
    clip_extractor = models["clip"]
    mindeye2 = models["mindeye2"]
    vae = models["vae"]
    cnx = models["cnx"]
    l1 = models["l1"]
    optimizer = optimizer
    lr_scheduler = lr_scheduler
 
  
    losses, lrs = [], [] # log list
    progress_bar = tqdm(range(0, num_epochs), ncols=50)
    for epoch in progress_bar:
        
        # ê¸°ë³¸ log
        loss_prior_sum = 0.0  # prior lossì˜ ëˆ„ì í•© -> í‰ê·  êµ¬í•  ë•Œ ì“°ì„
        loss_nce_sum = 0.0 # Negative Contrastive Estimation lossì˜ ëˆ„ì í•© -> í‰ê·  êµ¬í•  ë•Œ ì“°ì„
        loss_blurry_sum = 0.0 # l1 lossì˜ ëˆ„ì í•© -> í‰ê·  êµ¬í•  ë•Œ ì“°ì„
        loss_blurry_cont_sum = 0.0 # contlossì˜ ëˆ„ì í•© -> í‰ê·  êµ¬í•  ë•Œ ì“°ì„

        #### train #### 
        mindeye2.train()
        for index, batch in enumerate(train_data): # enumerate: indexì™€ ê°’ì„ ê°™ì´ ë°˜í™˜
            # global step ê³„ì‚°
            global_step = epoch * len(train_data) + index

            optimizer.zero_grad()

            fmri_list, image_list = {}, {}
            perm_list, betas_list, select_list = {}, {}, {}
            use_mix = epoch < int(mixup_pct * num_epochs)

            for subj in subj_names:
                fmri_vol, img = batch[subj]          
                fmri_vol = fmri_vol.to(device, non_blocking=True)
                img = img.to(device,  non_blocking=True)
                
                # epochì˜ 1/3 ì§€ì  ê¹Œì§€ë§Œ mixup ì‚¬ìš©
                if use_mix:
                    fmri_vol, perm, betas, select = mixup(fmri_vol)
                    perm_list[subj] = perm
                    betas_list[subj] = betas
                    select_list[subj] = select

                fmri_list[subj] = fmri_vol
                image_list[subj] = img

            image = torch.cat([image_list[subj] for subj in subj_names], dim=0)      
            if use_mix:
                perm   = torch.cat([perm_list[subj] for subj in subj_names], dim=0)
                betas  = torch.cat([betas_list[subj] for subj in subj_names], dim=0)
                select = torch.cat([select_list[subj] for subj in subj_names], dim=0)    

            # ê³ í•´ìƒë„ ì´ë¯¸ì§€ ì¦ê°•
            image = img_augment_high(image)

            with autocast(dtype=torch.bfloat16):
                #### forward ê³„ì‚° + loss ê³„ì‚° ####
                with torch.no_grad():
                    # target ì •ì˜
                    clip_target = clip_extractor(image)[0]

                # Shared-subject latent space(each -> 4096)
                voxel_ridge_list = [mindeye2.ridge(fmri_list[subj], i) for i, subj in enumerate(subj_names)]
                voxel_ridge = torch.cat(voxel_ridge_list, dim=0)

                # Residual MLP backbone 
                voxel_backbone, voxel_retrieval, voxel_lowlevels = mindeye2.backbone(voxel_ridge)

                # forward(Diffusion prior) -> prior loss
                loss_prior, _ = mindeye2.diffusion_prior(text_embed=voxel_backbone, image_embed=clip_target)
                
                # forward(retrieval submodule) -> contrstive loss(mixco_nce_loss + soft_clip_loss)
                clip_voxels_norm = nn.functional.normalize(voxel_retrieval.flatten(1), dim=-1).float()
                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1).float()
                # mixco_nce_loss(1/3) + soft_loss_temps(2/3)
                if epoch < int(mixup_pct * num_epochs):
                    nce_loss = mixco_nce_loss(
                        clip_voxels_norm,
                        clip_target_norm,
                        temp=.006, 
                        perm=perm, betas=betas, select=select)
                else:
                    soft_loss_temps = cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))
                    epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]
                    nce_loss = soft_clip_loss(
                        clip_voxels_norm,
                        clip_target_norm,
                        temp=epoch_temp)
                
                # forward(low-level submodule) -> l1_loss + cnx_loss
                image_enc_pred, transformer_feats = voxel_lowlevels

                with torch.no_grad():
                    image_enc = vae.encode(2*image-1).latent_dist.mode() * 0.18215
                loss_blurry = l1(image_enc_pred, image_enc)

                with torch.no_grad():
                    mean = torch.tensor([0.485, 0.456, 0.406]).to(device).reshape(1,3,1,1) # imagenetì˜ mean
                    std = torch.tensor([0.228, 0.224, 0.225]).to(device).reshape(1,3,1,1) # imagenetì˜ std
                    image_norm = (image - mean)/std
                    image_aug = (img_augment_low(image) - mean)/std
                    _, cnx_embeds = cnx(image_norm)
                    _, cnx_aug_embeds = cnx(image_aug)
                cont_loss = soft_cont_loss(
                    nn.functional.normalize(transformer_feats.reshape(-1, transformer_feats.shape[-1]), dim=-1),
                    nn.functional.normalize(cnx_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),
                    nn.functional.normalize(cnx_aug_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),
                    temp=0.2
                )

                # ìµœì¢… loss ì •ì˜
                loss = (prior_loss_coefficient * loss_prior) + (nce_loss_coefficient * nce_loss) + (lowlevel_loss_coefficient * (loss_blurry + 0.1*cont_loss)) 

                # NaN ì²´í¬ + ë„˜ê¹€
                if check_nan_and_log(global_step=index, fmri_vol=fmri_vol, voxel_backbone=voxel_backbone, voxel_retrieval=voxel_retrieval, voxel_lowlevels=voxel_lowlevels, loss=loss):
                    continue

            #### backward ê³„ì‚° + update ####
            # gradient ê³„ì‚° - ampì‚¬ìš©
            scaler.scale(loss).backward() # ampì‚¬ìš©

            # optimizer update - ampì‚¬ìš©
            scaler.step(optimizer) # ampì‚¬ìš©
            scaler.update() # ampì‚¬ìš©

            # learning rate schedule update
            lr_scheduler.step()

            #### log ####
            # loss, lr ë‹´ì•„ë‘ê¸°
            losses.append(loss.item())
            lrs.append(optimizer.param_groups[0]['lr'])

            # loss ëˆ„ì í•©
            loss_prior_sum += loss_prior.item()
            loss_nce_sum += nce_loss.item()
            loss_blurry_sum += loss_blurry.item()
            loss_blurry_cont_sum += cont_loss.item()

            logs = {
                # ê¸°ë³¸ í•™ìŠµ ìƒíƒœ
                "epoch": epoch,
                "train/num_steps": index + 1,  # í˜„ì¬ iteration
                "train/lr": lrs[-1],
                "train/global_step": global_step,
                "train/epoch": epoch,
                "train/loss": losses[-1],
                "train/loss_nce": nce_loss.item(),
                "train/loss_prior": loss_prior.item(),
                "train/loss_blurry": loss_blurry.item(),
                "train/loss_cont": cont_loss.item(),

                # ë””ë²„ê·¸: fmri_vol
                "debug/fmri_nan": float(torch.isnan(fmri_vol).any().item()),
                "debug/fmri_min": fmri_vol.min().item(),
                "debug/fmri_max": fmri_vol.max().item(),

                # ë””ë²„ê·¸: voxel_backbone
                "debug/voxel_backbone_nan": float(torch.isnan(voxel_backbone).any().item()),
                "debug/voxel_backbone_min": voxel_backbone.min().item(),
                "debug/voxel_backbone_max": voxel_backbone.max().item(),

                # ë””ë²„ê·¸: voxel_retrieval
                "debug/voxel_retrieval_nan": float(torch.isnan(voxel_retrieval).any().item()),
                "debug/voxel_retrieval_min": voxel_retrieval.min().item(),
                "debug/voxel_retrieval_max": voxel_retrieval.max().item(),

                # ë””ë²„ê·¸: voxel_lowlevels[0] (image_enc_pred)
                "debug/voxel_lowlevels_pred_nan": float(torch.isnan(voxel_lowlevels[0]).any().item()),
                "debug/voxel_lowlevels_pred_min": voxel_lowlevels[0].min().item(),
                "debug/voxel_lowlevels_pred_max": voxel_lowlevels[0].max().item(),

                # ë””ë²„ê·¸: loss ê°’ NaN ì—¬ë¶€
                "debug/loss_nan": float(torch.isnan(loss).item()),
            }
            progress_bar.set_postfix(**logs) # cliì— ì‹œê°í™”
            wandb.log(logs, step=global_step) # wandbì— ì‹œê°í™”

        if epoch >= 130 and epoch % 5 == 0:
        # if epoch % 10 == 0:
            save_path = os.path.join(args.root_dir, args.code_dir, args.output_dir, "mindeye2_metric", f"mindeye2_finetunning_{epoch}_{experiment_name}.pt")
            save_path = get_unique_path(save_path)
            torch.save(mindeye2.state_dict(), save_path)

        torch.cuda.empty_cache() # gpu ë©”ëª¨ë¦¬ cacheì‚­ì œ
        gc.collect() # # gpu ë©”ëª¨ë¦¬ ì•ˆ ì“°ëŠ”ê±° ì‚­ì œ



def inference_evaluate(args, subj_names, test_data, models, metrics, ckpt_dir):

    # train argument
    device = args.device
    experiment_name = args.experiment_name
    num_epochs = args.num_epochs
    subj_names = subj_names

    # test argument
    seed = args.seed
    scaler = GradScaler(enabled=False) # autocast scaler ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

    # model ì •ì˜
    mindeye2 = models["mindeye2"]
    vae = models["vae"]
    clip_linear = models["clip_linear"]
    clip_text_model = models["clip_text_model"]
    token_to_text = models["token_to_text"]
    sdxl_unclip = models["sdxl_unclip"]
    base_text_embedder1 = models["base_text_embedder1"]
    base_text_embedder2 = models["base_text_embedder2"]
    sdxl = models["sdxl"]
    noise_scheduler = models["noise_scheduler"]

    # checkpoint ë¶ˆëŸ¬ì˜¤ê¸°
    ckpt_files = sorted([os.path.join(ckpt_dir, f) for f in os.listdir(ckpt_dir) if f.endswith(".pt") and "mindeye2_pretrain_continous" in f], key=lambda x: int(re.search(r"_(\d+)_\d+\.pt", os.path.basename(x)).group(1)) if re.search(r"_(\d+)_\d+\.pt", os.path.basename(x)) else 0, reverse=True)
    for ckpt_path in ckpt_files:
        print(f"Loading checkpoint from {ckpt_path}")

        # íŒŒì¼ëª…ì—ì„œ ìˆ«ì ì¸ë±ìŠ¤ ì¶”ì¶œ
        ckpt_name = os.path.basename(ckpt_path)
        match = re.search(r"_(\d+)_", ckpt_name)
        ckpt_num = int(match.group(1))

        #### inference ####
        all_targets = []
        all_targets_ids = []
        all_recons = []
        all_blurryrecons = []
        all_captions = []
        all_enhanced_recons = []
        all_final_recons = []

        mindeye2.eval()
        mindeye2.load_state_dict(torch.load(ckpt_path, map_location="cpu"))
        # ë‚˜ë¨¸ì§€ subject(2,5,7) weight ì‚­ì œ (ë©”ëª¨ë¦¬ ì ˆì•½)
        # del mindeye2.ridge.linears[1:]
        print(f"[ë””ë²„ê·¸] ridge ë‚´ë¶€ linears ê°œìˆ˜: {len(mindeye2.ridge.linears)}")
        print(f"[ë””ë²„ê·¸] ridge ë‚´ë¶€ layer êµ¬ì¡°:")
        print(f"  {[f'linears.{i}: ({layer.in_features} â†’ {layer.out_features}, bias={layer.bias is not None})' for i, layer in enumerate(mindeye2.ridge.linears)]}")

        progress_bar = tqdm(enumerate(test_data), ncols=120)
        for index, batch in progress_bar:
            with torch.inference_mode():
                generator = torch.Generator(device=device)
                generator.manual_seed(seed)
                
                # sub-01ë§Œ ìˆìŒ
                fmri_list = {}
                for subj in subj_names:
                    fmri_vol, img, image_id = batch[subj]          
                    fmri_vol = fmri_vol.to(device, non_blocking=True)
                    fmri_list[subj] = fmri_vol

                    # imageì™€ image_idëŠ” dict í˜•íƒœë¡œ ë‹´ì•„ë‘ê¸°
                    for i, img_id in zip(img, image_id):
                        tgt = transforms.Resize((256, 256), antialias=True)(i.cpu()).float()
                        all_targets.append(tgt)
                        all_targets_ids.append(img_id)
                       

                # Shared-subject latent space(each -> 4096)
                voxel_ridge_list = [mindeye2.ridge(fmri_list[subj], i) for i, subj in enumerate(subj_names)]
                voxel_ridge = torch.cat(voxel_ridge_list, dim=0)

                # Residual MLP backbone(4096 -> 256*1664)
                voxel_backbone, voxel_retrieval, voxel_lowlevels = mindeye2.backbone(voxel_ridge)

                # Diffusion prior(256*1664)
                loss_prior = mindeye2.diffusion_prior.p_sample_loop(voxel_backbone.shape, text_cond = dict(text_embed = voxel_backbone), timesteps = 20 , cond_scale = 1.)

                # SDXL unCLIP
                template = {
                    "jpg": torch.randn(args.inference_batch_size, 3, 1, 1).to(device),             # (B,3,1,1)
                    "original_size_as_tuple": torch.ones(args.inference_batch_size, 2).to(device) * 768,  # (B,2)
                    "crop_coords_top_left": torch.zeros(args.inference_batch_size, 2).to(device)          # (B,2)
                }
                out = sdxl_unclip.conditioner(template)
                vector_suffix = out["vector"][:, :1024].to(device) # vector(ì „ì—­ì¡°ê±´), crossattn(ì• ë¶€ë¶„)ì´ í•„ìš” ì—†ì–´ì„œ 1024-dimë¶€ë¶„ë§Œ ì‚¬ìš©

                samples = unclip_recon(loss_prior, sdxl_unclip, vector_suffix, num_samples=1, device=device)

                # caption linear + caption ìƒì„±
                pred_caption_emb = clip_linear(loss_prior)
                generated_ids = clip_text_model.generate(pixel_values=pred_caption_emb, max_length=20)
                generated_caption = token_to_text.batch_decode(generated_ids, skip_special_tokens=True)
                
                # low-level submodule
                image_enc_pred, transformer_feats = voxel_lowlevels
                blurred_images = (vae.decode(image_enc_pred/0.18215).sample/ 2 + 0.5).clamp(0,1)
                
                # SDXL unCLIP + caption + low-level submoduleì„ ëª¨ì•„ì„œ ìµœì¢… ì¬êµ¬ì„±
                template2 = {
                    "txt": [""] * args.inference_batch_size,             
                    "original_size_as_tuple": torch.ones(args.inference_batch_size, 2).to(device) * 768,  # (B,2)
                    "crop_coords_top_left": torch.zeros(args.inference_batch_size, 2).to(device),  
                    "target_size_as_tuple": torch.zeros(args.inference_batch_size, 2).to(device) * 1024
                }
                out2 = sdxl.conditioner(template2)
                crossattn_c = out2["crossattn"].to(device) # cfgí•  ë•Œ ì‚¬ìš©
                vector_c = out2["vector"][:,-1536:].to(device) # cfgí•  ë•Œ ì‚¬ìš©

                negative_prompt = (
                    "painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, "
                    "deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, "
                    "skinny, glitchy, double torso, extra arms, extra hands, mangled fingers, "
                    "missing lips, ugly face, distorted face, extra legs, anime"
                )
                templete2_uc = {
                    "txt": [negative_prompt] * args.inference_batch_size,           
                    "original_size_as_tuple": torch.ones(args.inference_batch_size, 2).to(device) * 768,  # (B,2)
                    "crop_coords_top_left": torch.zeros(args.inference_batch_size, 2).to(device),  
                    "target_size_as_tuple": torch.zeros(args.inference_batch_size, 2).to(device) * 1024
                }
                out2_uc = sdxl.conditioner(templete2_uc)
                crossattn_uc = out2_uc["crossattn"].to(device) # cfgí•  ë•Œ ì‚¬ìš©
                vector_uc = out2_uc["vector"].to(device) # cfgí•  ë•Œ ì‚¬ìš©

                enhanced_samples = sdxl_recon(args.inference_batch_size, samples, generated_caption, sdxl, base_text_embedder1, base_text_embedder2, vector_c, crossattn_uc, vector_uc, num_samples=1, img2img_timepoint=13, device=device)
                final_recons = enhanced_samples*.75 + blurred_images*.25

                for b in range(samples.shape[0]):
                    all_recons.append(transforms.Resize((256,256), antialias=True)(samples[b].cpu()).float())
                    all_blurryrecons.append(transforms.Resize((256,256), antialias=True)(blurred_images[b].cpu()).float())
                    all_captions.append(generated_caption[b])
                    all_enhanced_recons.append(transforms.Resize((256,256), antialias=True)(enhanced_samples[b].cpu()).float())
                    all_final_recons.append(transforms.Resize((256,256), antialias=True)(final_recons[b].cpu()).clamp(0,1).float())

                torch.cuda.empty_cache() # gpu ë©”ëª¨ë¦¬ cacheì‚­ì œ
                gc.collect() # gpu ë©”ëª¨ë¦¬ ì•ˆ ì“°ëŠ”ê±° ì‚­ì œ

        all_recons = torch.stack(all_recons, dim=0)  # [N, 3, H, W]
        all_blurryrecons = torch.stack(all_blurryrecons, dim=0)  # [N, 3, H, W]
        all_enhanced_recons = torch.stack(all_enhanced_recons, dim=0)  # [N, 3, H, W]
        all_final_recons = torch.stack(all_final_recons, dim=0)  # [N, 3, H, W]
        all_targets = torch.stack(all_targets, dim=0)  # [3, H, W]ì—¬ëŸ¬ê°œ -> [N, 3, H, W]

        # ğŸ” ë””ë²„ê·¸ ì¶œë ¥
        print("\n========== Debug: Tensor Shapes ==========")
        print(f"all_recons shape         : {tuple(all_recons.shape)}")
        print(f"all_blurryrecons shape   : {tuple(all_blurryrecons.shape)}")
        print(f"all_enhanced_recons shape: {tuple(all_enhanced_recons.shape)}")
        print(f"all_final_recons shape   : {tuple(all_final_recons.shape)}")
        print(f"all_targets shape        : {tuple(all_targets.shape)}")
        # í˜¹ì‹œ ì±„ë„ ìˆ˜ë‚˜ dtypeì´ ì•ˆ ë§ì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ì¶”ê°€ë¡œ í™•ì¸
        print("\n========== Debug: Tensor Details ==========")
        print(f"all_final_recons dtype: {all_final_recons.dtype}, range=({all_final_recons.min():.3f}, {all_final_recons.max():.3f})")
        print(f"all_targets dtype     : {all_targets.dtype}, range=({all_targets.min():.3f}, {all_targets.max():.3f})")
        print("===========================================\n")   

        #### evaluate ####
        results = {}

        compare_sets = {
            "final_recons": all_final_recons,
            "enhanced_recons": all_enhanced_recons,
            "recons": all_recons,
            "blurryrecons": all_blurryrecons
        }

        for name, recons in compare_sets.items():
            # ê° ë¹„êµ ì„¸íŠ¸ë³„ metric ê³„ì‚°
            sub_results = {}

            # PixCorr / SSIM
            sub_results["PixCorr"] = metrics["pixcorr"](recons, all_targets)
            sub_results["SSIM"] = metrics["ssim"](recons, all_targets)

            # AlexNet
            sub_results["AlexNet_2"] = metrics["alexnet2"]["metric_fn"](
                args, recons, all_targets,
                metrics["alexnet2"]["model"],
                metrics["alexnet2"]["preprocess"],
                metrics["alexnet2"]["layer"]
            )
            sub_results["AlexNet_5"] = metrics["alexnet5"]["metric_fn"](
                args, recons, all_targets,
                metrics["alexnet5"]["model"],
                metrics["alexnet5"]["preprocess"],
                metrics["alexnet5"]["layer"]
            )

            # CLIP / Inception / EfficientNet / SwAV
            sub_results["CLIP"] = metrics["clip"]["metric_fn"](
                args, recons, all_targets,
                metrics["clip"]["model"],
                metrics["clip"]["preprocess"]
            )
            sub_results["Inception"] = metrics["inception"]["metric_fn"](
                args, recons, all_targets,
                metrics["inception"]["model"],
                metrics["inception"]["preprocess"]
            )
            sub_results["EfficientNet_B1"] = metrics["efficientnet"]["metric_fn"](
                args, recons, all_targets,
                metrics["efficientnet"]["model"],
                metrics["efficientnet"]["preprocess"]
            )
            sub_results["SwAV"] = metrics["swav"]["metric_fn"](
                args, recons, all_targets,
                metrics["swav"]["model"],
                metrics["swav"]["preprocess"]
            )

            results[name] = sub_results

            print(f"\n===== {name} vs Target =====")
            for metric_name, score in sub_results.items():
                print(f"{metric_name:15}: {score:.4f}")
            print("=" * 40)

        # wandbì— ê¸°ë¡ (í‰ê°€ ì„¸íŠ¸ë³„ë¡œ êµ¬ë¶„)
        for recon_name, sub_results in results.items():
            wandb.log(
                {f"eval/{recon_name}/epoch{ckpt_num}_{k}": v for k, v in sub_results.items()},
                step=ckpt_num
            )

        # CLIP ì ìˆ˜ ê¸°ì¤€ (final_recons ê¸°ì¤€)
        current_score = results["final_recons"].get("CLIP", 0.0)
        if current_score > 0.7:

            # save_recons ì €ì¥
            recons_dir = os.path.join(args.root_dir, args.code_dir, args.output_dir, "mindeye2_metric", "recon_benchmark")
            save_gt_vs_recon_images_extended(all_targets, all_recons, all_blurryrecons, all_enhanced_recons, all_final_recons, all_targets_ids, save_dir=recons_dir, layout='horizontal')

            # ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
            result_path = os.path.join(args.root_dir, args.code_dir, args.output_dir, "mindeye2_metric", f"mindeye2_finetunning_metrics_{ckpt_num}_{experiment_name}.txt")
            result_path = get_unique_path(result_path)
            with open(result_path, "w") as f:
                for recon_name, sub_results in results.items():
                    f.write(f"==== {recon_name} vs Target ====\n")
                    for metric_name, score in sub_results.items():
                        f.write(f"{metric_name}: {score:.4f}\n")
                    f.write("\n")

        torch.cuda.empty_cache() # gpu ë©”ëª¨ë¦¬ cacheì‚­ì œ
        gc.collect() # gpu ë©”ëª¨ë¦¬ ì•ˆ ì“°ëŠ”ê±° ì‚­ì œ







                